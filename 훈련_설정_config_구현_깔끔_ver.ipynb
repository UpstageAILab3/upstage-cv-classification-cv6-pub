{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.26.0)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
            "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.16.0)\n",
            "Collecting torchmetrics (from -r requirements.txt (line 10))\n",
            "  Obtaining dependency information for torchmetrics from https://files.pythonhosted.org/packages/29/1b/b38033e61c28e52dde7bd459df6567c04c127ee153722c73b9acd0fe550b/torchmetrics-1.4.1-py3-none-any.whl.metadata\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.0.8)\n",
            "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.17.5)\n",
            "Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (2023.9.2)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 9)) (2.31.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->-r requirements.txt (line 10))\n",
            "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/ea/d5/ed204bc738672c17455019b5e0c7c8d1effb0ea17707150ca50336298ca0/lightning_utilities-0.11.6-py3-none-any.whl.metadata\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (8.0.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (4.1.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (5.27.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (5.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (2.12.0)\n",
            "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (68.0.0)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 13)) (0.22.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 13)) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 13)) (4.8.1.78)\n",
            "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (2023.7.22)\n",
            "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 13)) (2.33.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 13)) (2023.12.9)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 13)) (0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 8)) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12)) (5.0.1)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.6 torchmetrics-1.4.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "#pip install pillow numpy pandas seaborn matplotlib tqdm scikit-learn torch torchvision torchmetrics timm wandb albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "라이브러리 목록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "import json\n",
        "import math\n",
        "import timm\n",
        "import time\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import albumentations as album\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchmetrics import F1Score, classification\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "기본세팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 경로 및 기본설정\n",
        "base_path = \"\"\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# FineTuning 대상 변수 미리 꺼내놓기\n",
        "num_classes = 17\n",
        "learning_rate = 1e-3\n",
        "optimizer = \"Ranger\"\n",
        "img_size = 224\n",
        "batch_size = 4\n",
        "gradient_clip_val = 1  # 0이면 클리핑 안함\n",
        "use_mixed_precision = True\n",
        "focal_alpha = [1] * 17\n",
        "focal_alpha[6] = 3\n",
        "focal_alpha[10] = 3\n",
        "focal_alpha[14] = 3\n",
        "k_fold = 5  # 0이면 안함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "focal_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "백본 결정하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tiny_vit_5m_224.dist_in22k',\n",
              " 'tiny_vit_5m_224.dist_in22k_ft_in1k',\n",
              " 'tiny_vit_5m_224.in1k',\n",
              " 'tiny_vit_11m_224.dist_in22k',\n",
              " 'tiny_vit_11m_224.dist_in22k_ft_in1k',\n",
              " 'tiny_vit_11m_224.in1k',\n",
              " 'tiny_vit_21m_224.dist_in22k',\n",
              " 'tiny_vit_21m_224.dist_in22k_ft_in1k',\n",
              " 'tiny_vit_21m_224.in1k',\n",
              " 'tiny_vit_21m_384.dist_in22k_ft_in1k',\n",
              " 'tiny_vit_21m_512.dist_in22k_ft_in1k',\n",
              " 'tinynet_a.in1k',\n",
              " 'tinynet_b.in1k',\n",
              " 'tinynet_c.in1k',\n",
              " 'tinynet_d.in1k',\n",
              " 'tinynet_e.in1k']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 고르시오 tiny, mobilenetv4 GMACs, GFLOPS따져서\n",
        "timm.list_models(\"tiny*\", pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['caformer_b36.sail_in1k',\n",
              " 'caformer_b36.sail_in1k_384',\n",
              " 'caformer_b36.sail_in22k',\n",
              " 'caformer_b36.sail_in22k_ft_in1k',\n",
              " 'caformer_b36.sail_in22k_ft_in1k_384',\n",
              " 'caformer_m36.sail_in1k',\n",
              " 'caformer_m36.sail_in1k_384',\n",
              " 'caformer_m36.sail_in22k',\n",
              " 'caformer_m36.sail_in22k_ft_in1k',\n",
              " 'caformer_m36.sail_in22k_ft_in1k_384',\n",
              " 'caformer_s18.sail_in1k',\n",
              " 'caformer_s18.sail_in1k_384',\n",
              " 'caformer_s18.sail_in22k',\n",
              " 'caformer_s18.sail_in22k_ft_in1k',\n",
              " 'caformer_s18.sail_in22k_ft_in1k_384',\n",
              " 'caformer_s36.sail_in1k',\n",
              " 'caformer_s36.sail_in1k_384',\n",
              " 'caformer_s36.sail_in22k',\n",
              " 'caformer_s36.sail_in22k_ft_in1k',\n",
              " 'caformer_s36.sail_in22k_ft_in1k_384']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 고르시오\n",
        "timm.list_models(\"cafo*\", pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"caformer_s36.sail_in22k_ft_in1k\"\n",
        "model_name = \"caformer_m36.sail_in22k_ft_in1k\"  # \"tiny_vit_11m_224.dist_in22k_ft_in1k\"\n",
        "# model_name = \"tiny_vit_11m_224.dist_in22k_ft_in1k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"k-fold-cross-validation-pytorch\"\n",
        "\n",
        "\n",
        "def get_short_model_name(model_name):\n",
        "    return model_name.split(\".\")[0]\n",
        "\n",
        "\n",
        "model_short_name = get_short_model_name(model_name)\n",
        "GROUP_NAME = f\"{model_short_name}_{optimizer}_{wandb.util.generate_id()[:8]}\"\n",
        "JOB_TYPE = \"train\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Config 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_manual = {\n",
        "    # 기본 설정\n",
        "    \"seed\": 42,\n",
        "    \"device\": device,\n",
        "    \"num_workers\": 0,\n",
        "    \"group_name\": GROUP_NAME,\n",
        "    \"base_path\": base_path,\n",
        "    \"project_name\": PROJECT_NAME,\n",
        "    # 데이터 관련 설정\n",
        "    \"batch_size\": batch_size,\n",
        "    \"img_size\": img_size,\n",
        "    \"num_classes\": num_classes,\n",
        "    # 모델 관련 설정\n",
        "    \"model_name\": model_name,  # 예시\n",
        "    \"pretrained\": True,\n",
        "    # 훈련 관련 설정\n",
        "    \"epochs\": 300,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"k_fold\": k_fold,\n",
        "    # 손실 함수 관련 설정\n",
        "    \"loss\": \"smoothfocal\",  # 'ce', 'focal', 'smoothce', 'smoothfocal'\n",
        "    \"focal_gamma\": 2,\n",
        "    \"smoothing\": 0.1,\n",
        "    \"focal_alpha\": focal_alpha,\n",
        "    # 데이터 증강 관련 설정\n",
        "    \"mixup_alpha\": 1.0,\n",
        "    \"cutmix_alpha\": 1.0,\n",
        "    \"mixup_prob\": 0.2,\n",
        "    \"cutmix_prob\": 0,\n",
        "    # 옵티마이저 설정\n",
        "    \"optimizer\": optimizer,  # 'SGD', 'Adam', 'AdamW', 'RMSprop', 'Adadelta', 'Ranger'\n",
        "    \"momentum\": 0.9,\n",
        "    # Adam과 AdamW 특정 설정\n",
        "    \"adam_beta1\": 0.9,\n",
        "    \"adam_beta2\": 0.999,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    # RMSprop 특정 설정\n",
        "    \"rmsprop_alpha\": 0.99,\n",
        "    \"rmsprop_epsilon\": 1e-8,\n",
        "    # Adadelta 특정 설정\n",
        "    \"adadelta_rho\": 0.9,\n",
        "    \"adadelta_epsilon\": 1e-6,\n",
        "    # Ranger 특정 설정\n",
        "    \"ranger_alpha\": 0.5,\n",
        "    \"ranger_k\": 6,\n",
        "    \"ranger_N_sma_threshhold\": 5,\n",
        "    \"ranger_beta1\": 0.95,\n",
        "    \"ranger_beta2\": 0.999,\n",
        "    \"ranger_eps\": 1e-5,\n",
        "    \"ranger_use_gc\": True,\n",
        "    \"ranger_gc_conv_only\": False,\n",
        "    # 스케줄러 설정\n",
        "    \"scheduler\": \"CosineAnnealingWarmRestarts\",  # 'LambdaLR', 'MultiplicativeLR', 'StepLR', 'CosineAnnealingLR',\n",
        "    #'ReduceLROnPlateau', 'CyclicLR', 'OneCycleLR', 'CosineAnnealingWarmRestarts'\n",
        "    \"verbose\": False,\n",
        "    # LambdaLR 설정\n",
        "    \"lambda_factor\": 0.95,\n",
        "    \"last_epoch\": -1,\n",
        "    # StepLR 설정\n",
        "    \"step_size\": 10,\n",
        "    \"gamma\": 0.5,\n",
        "    # ReduceLROnPlateau 설정\n",
        "    \"mode\": \"min\",\n",
        "    # CyclicLR 설정\n",
        "    \"base_lr\": 0.00005,\n",
        "    \"max_lr\": 0.0001,\n",
        "    \"step_size_up\": 5,\n",
        "    # OneCycleLR 설정\n",
        "    \"steps_per_epoch\": 10,\n",
        "    \"anneal_strategy\": \"linear\",\n",
        "    # CosineAnnealingLR 설정\n",
        "    \"T_max\": 50,\n",
        "    \"eta_min\": 0,\n",
        "    # CosineAnnealingWarmRestarts 설정\n",
        "    \"T_0\": 10,\n",
        "    \"T_mult\": 1,\n",
        "    # 추가 훈련 기법 설정\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"early_stopping_delta\": 0.001,\n",
        "    \"warmup_epochs\": 5,\n",
        "    \"warmup_factor\": 0.1,\n",
        "    \"use_mixed_precision\": use_mixed_precision,\n",
        "    \"gradient_clip_val\": gradient_clip_val,  # 0이면 안함\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wandb 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use wandb-core, temporary for wandb's new backend\n",
        "wandb.require(\"core\")\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "커스텀 Config 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Config({', '.join(f'{k}={v}' for k, v in self.__dict__.items())})\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.__dict__.items())\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return getattr(self, key)\n",
        "\n",
        "    def get(self, key, default=None):\n",
        "        return getattr(self, key, default)\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(**config_manual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, csv, path, transform=None):\n",
        "        self.df = pd.read_csv(csv).values\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name, target = self.df[idx]\n",
        "        img = np.array(PIL.Image.open(os.path.join(self.path, name)))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        return img, int(target)  # 타겟을 정수로 반환\n",
        "\n",
        "\n",
        "def get_transform(img_size, mean, std):\n",
        "    return album.Compose(\n",
        "        [\n",
        "            album.LongestMaxSize(max_size=img_size),\n",
        "            album.PadIfNeeded(\n",
        "                min_height=img_size, min_width=img_size, border_mode=0, value=(0, 0, 0)\n",
        "            ),\n",
        "            album.Normalize(mean=mean, std=std),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def create_data_set(\n",
        "    base_path, img_size=448, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "):\n",
        "    # 경로 설정\n",
        "    train_csv = os.path.join(base_path, \"aug_train.csv\")\n",
        "    train_dir = os.path.join(base_path, \"aug_train\")\n",
        "    # train_csv = os.path.join(base_path, \"train.csv\")\n",
        "    # train_dir = os.path.join(base_path, \"train\")\n",
        "\n",
        "    test_csv = os.path.join(base_path, \"sample_submission.csv\")\n",
        "    test_dir = os.path.join(base_path, \"test\")\n",
        "\n",
        "    # Dataset 정의\n",
        "    trn_dataset = ImageDataset(\n",
        "        train_csv, train_dir, transform=get_transform(img_size, mean, std)\n",
        "    )\n",
        "    tst_dataset = ImageDataset(\n",
        "        test_csv, test_dir, transform=get_transform(img_size, mean, std)\n",
        "    )\n",
        "\n",
        "    print(f\"Training dataset size: {len(trn_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(tst_dataset)}\")\n",
        "    return trn_dataset, tst_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 100480\n",
            "Test dataset size: 3140\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.init()\n",
        "# 데이터셋 준비, 검증셋을 따로 준비할수도 있다.\n",
        "train_ds, test_dataset = create_data_set(base_path, config.img_size)\n",
        "train_size = int(0.9 * len(train_ds))\n",
        "val_size = len(train_ds) - train_size\n",
        "train_dataset, validation_dataset = random_split(train_ds, [train_size, val_size])\n",
        "# DataLoader 정의\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "validation_loader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "메트릭"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomMetrics(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.f1_macro = F1Score(\n",
        "            task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        "        )\n",
        "        self.accuracy_per_class = classification.MulticlassAccuracy(\n",
        "            num_classes=num_classes, average=None\n",
        "        )\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        # preds: (batch_size, num_classes)\n",
        "        # target: (batch_size,)\n",
        "        if preds.dim() == 2:\n",
        "            preds = preds.argmax(dim=1)\n",
        "        # F1 Macro 계산\n",
        "        f1_macro = self.f1_macro(preds, target)\n",
        "\n",
        "        # 각 클래스별 Accuracy 계산\n",
        "        accuracies = self.accuracy_per_class(preds, target)\n",
        "\n",
        "        # 가장 낮은 Accuracy 찾기\n",
        "        lowest_accuracy = torch.min(accuracies)\n",
        "\n",
        "        return {\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"lowest_class_accuracy\": lowest_accuracy,\n",
        "            \"accuracies_per_class\": accuracies,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = CustomMetrics(config.num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TTA 모듈 -> 차후 분석위한 분석툴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TTA:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        config,\n",
        "        threshold=0.95,\n",
        "        temperature=0.5,\n",
        "        tta_transforms=[\n",
        "            transforms.Lambda(lambda x: x),  # 원본\n",
        "            transforms.Lambda(\n",
        "                lambda x: transforms.functional.rotate(x, 90)\n",
        "            ),  # 90도 회전\n",
        "            transforms.Lambda(\n",
        "                lambda x: transforms.functional.rotate(x, 180)\n",
        "            ),  # 180도 회전\n",
        "            transforms.Lambda(\n",
        "                lambda x: transforms.functional.rotate(x, 270)\n",
        "            ),  # 270도 회전\n",
        "            transforms.Lambda(lambda x: transforms.functional.hflip(x)),  # 수평 뒤집기\n",
        "            transforms.Lambda(lambda x: transforms.functional.vflip(x)),  # 수직 뒤집기\n",
        "            transforms.Compose(\n",
        "                [  # 90도 회전 + 수평 뒤집기\n",
        "                    transforms.Lambda(lambda x: transforms.functional.rotate(x, 90)),\n",
        "                    transforms.Lambda(lambda x: transforms.functional.hflip(x)),\n",
        "                ]\n",
        "            ),\n",
        "            transforms.Compose(\n",
        "                [  # 270도 회전 + 수평 뒤집기\n",
        "                    transforms.Lambda(lambda x: transforms.functional.rotate(x, 270)),\n",
        "                    transforms.Lambda(lambda x: transforms.functional.hflip(x)),\n",
        "                ]\n",
        "            ),\n",
        "        ],\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.threshold = threshold\n",
        "        self.temperature = temperature\n",
        "        self.tta_transforms = tta_transforms\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def tta_inference(self, image):\n",
        "        self.model.eval()\n",
        "        probs = []\n",
        "        for transform in self.tta_transforms:\n",
        "            augmented = transform(image)\n",
        "            output = self.model(augmented.unsqueeze(0).to(self.device))\n",
        "            prob = F.softmax(output, dim=1)\n",
        "            probs.append(prob.squeeze().cpu())\n",
        "        return torch.stack(probs)\n",
        "\n",
        "    def aggregate_predictions(self, probs, method):\n",
        "        if method == \"mean\":\n",
        "            result = probs.mean(dim=0)\n",
        "        elif method == \"max\":\n",
        "            result = probs.max(dim=0)[0]\n",
        "        elif method == \"temp_sharpen\":\n",
        "            result = self._temp_sharpen(probs, self.temperature)\n",
        "        elif method == \"mode\":\n",
        "            result = self._mode(probs)\n",
        "        elif method == \"modethreshold\":\n",
        "            result = self._mode_threshold(probs, self.threshold)\n",
        "        elif method == \"no_tta\":\n",
        "            result = probs[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported TTA aggregation method: {method}\")\n",
        "\n",
        "        # 결과를 softmax 처리\n",
        "        return F.softmax(result, dim=0)\n",
        "\n",
        "    def _temp_sharpen(self, probs, temperature=0.5):\n",
        "        sharpened = probs ** (1 / temperature)\n",
        "        return sharpened.mean(dim=0) / sharpened.mean(dim=0).sum()\n",
        "\n",
        "    def _mode(self, probs):\n",
        "        labels = probs.argmax(dim=1)\n",
        "        mode = Counter(labels.tolist()).most_common(1)[0][0]\n",
        "        result = torch.zeros(probs.shape[1])\n",
        "        result[mode] = 1\n",
        "        return result\n",
        "\n",
        "    def _mode_threshold(self, probs, threshold=0.95):\n",
        "        high_conf = (probs > threshold).any(dim=1)\n",
        "        if high_conf.any():\n",
        "            high_conf_labels = probs[high_conf].argmax(dim=1)\n",
        "            mode = Counter(high_conf_labels.tolist()).most_common(1)[0][0]\n",
        "        else:\n",
        "            mode = Counter(probs.argmax(dim=1).tolist()).most_common(1)[0][0]\n",
        "        result = torch.zeros(probs.shape[1])\n",
        "        result[mode] = 1\n",
        "        return result\n",
        "\n",
        "    def _ensemble_predictions(self, all_preds):\n",
        "        df = pd.DataFrame(all_preds)\n",
        "        mode_preds = df.mode(axis=1)\n",
        "\n",
        "        ensemble_preds = []\n",
        "        for i in range(len(df)):\n",
        "            if len(mode_preds.iloc[i].dropna()) == 1:\n",
        "                # 동점이 없는 경우\n",
        "                ensemble_preds.append(mode_preds.iloc[i, 0])\n",
        "            else:\n",
        "                # 동점이 있는 경우\n",
        "                mode_group = set(mode_preds.iloc[i].dropna())\n",
        "                if df.loc[i, \"max\"] in mode_group:\n",
        "                    ensemble_preds.append(df.loc[i, \"max\"])\n",
        "                else:\n",
        "                    # max가 모드 그룹에 없을 경우 우선순위에 따라 선택\n",
        "                    for method in [\n",
        "                        \"temp_sharpen\",\n",
        "                        \"modethreshold\",\n",
        "                        \"mode\",\n",
        "                        \"mean\",\n",
        "                        \"no_tta\",\n",
        "                    ]:\n",
        "                        if df.loc[i, method] in mode_group:\n",
        "                            ensemble_preds.append(df.loc[i, method])\n",
        "                            break\n",
        "                    else:\n",
        "                        # 모든 방법이 모드 그룹에 없는 경우 그냥 max(거의 일어나지 않을 것임)\n",
        "                        ensemble_preds.append(mode_preds.iloc[i, \"max\"])\n",
        "        return ensemble_preds\n",
        "\n",
        "    def _save_dataframe_as_image(self, df, filename=\"df_accuracy_table.png\"):\n",
        "        fig, ax = plt.subplots(\n",
        "            figsize=(12, len(df) * 0.5 + 1)\n",
        "        )  # 행 수에 따라 세로 크기 조정\n",
        "        ax.axis(\"tight\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        table = ax.table(\n",
        "            cellText=df.values,\n",
        "            colLabels=df.columns,\n",
        "            rowLabels=df.index,\n",
        "            cellLoc=\"center\",\n",
        "            loc=\"center\",\n",
        "        )\n",
        "\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(10)\n",
        "        table.scale(1.2, 1.2)\n",
        "        plt.savefig(filename, bbox_inches=\"tight\", dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"Table saved as {filename}\")\n",
        "\n",
        "    def tta_predict(self, loader):\n",
        "        methods = [\"no_tta\", \"mean\", \"max\", \"temp_sharpen\", \"mode\", \"modethreshold\"]\n",
        "        all_preds = {method: [] for method in methods}\n",
        "        all_probs = {method: [] for method in methods}\n",
        "        all_targets = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(loader, desc=\"Generating predictions\"):\n",
        "                images = images.to(self.device)\n",
        "                all_targets.append(targets)\n",
        "                for image in images:\n",
        "                    probs = self.tta_inference(image)\n",
        "                    for method in methods:\n",
        "                        agg_prob = self.aggregate_predictions(probs, method)\n",
        "                        all_preds[method].append(agg_prob.argmax().item())\n",
        "                        all_probs[method].append(agg_prob.cpu().numpy())\n",
        "\n",
        "        all_targets_tensor = torch.cat(all_targets)\n",
        "        ensemble_preds = self._ensemble_predictions(all_preds)\n",
        "        all_preds[\"ensemble\"] = ensemble_preds\n",
        "        # Note: We don't add 'ensemble' to all_probs\n",
        "\n",
        "        return all_preds, all_probs, all_targets_tensor\n",
        "\n",
        "    def tta_evaluate(self, loader):\n",
        "        all_preds, all_probs, all_targets = self.tta_predict(loader)\n",
        "\n",
        "        results = {}\n",
        "        metrics = CustomMetrics(self.config.num_classes)\n",
        "\n",
        "        for method in all_preds.keys():\n",
        "            preds = torch.tensor(all_preds[method])\n",
        "            targets = all_targets.clone().detach()\n",
        "            metric_results = metrics(preds, targets)\n",
        "            results[method] = {\n",
        "                \"preds\": preds,\n",
        "                \"probs\": all_probs.get(method) if method != \"ensemble\" else None,\n",
        "                \"f1_macro\": metric_results[\"f1_macro\"].item(),\n",
        "                \"lowest_class_accuracy\": metric_results[\"lowest_class_accuracy\"].item(),\n",
        "                \"accuracies_per_class\": metric_results[\"accuracies_per_class\"].tolist(),\n",
        "            }\n",
        "\n",
        "        return results, all_targets\n",
        "\n",
        "    def save_predictions(self, predictions, probs, output_dir=\"submissions\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        sample_submission_path = os.path.join(\n",
        "            self.config.base_path, \"sample_submission.csv\"\n",
        "        )\n",
        "        sample_submission = pd.read_csv(sample_submission_path)\n",
        "\n",
        "        for method, preds in predictions.items():\n",
        "            # 예측 결과 저장\n",
        "            submission = sample_submission.copy()\n",
        "            submission[\"target\"] = preds\n",
        "            try:\n",
        "                pred_filename = f\"{self.config.group_name}_submission_tta_{method}.csv\"\n",
        "            except:\n",
        "                pred_filename = f\"data_orig_submission_tta_{method}.csv\"\n",
        "            pred_path = f\"{output_dir}/{pred_filename}\"\n",
        "            submission.to_csv(pred_path, index=False)\n",
        "            print(f\"Saved predictions for method: {method}\")\n",
        "\n",
        "            # 확률 저장\n",
        "            if method != \"ensemble\" and probs is not None:\n",
        "                prob_df = pd.DataFrame(\n",
        "                    probs[method],\n",
        "                    columns=[f\"prob_{i}\" for i in range(len(probs[method][0]))],\n",
        "                )\n",
        "                try:\n",
        "                    prob_filename = (\n",
        "                        f\"{self.config.group_name}_labelprob_tta_{method}.csv\"\n",
        "                    )\n",
        "                except:\n",
        "                    prob_filename = f\"data_orig_labelprob_tta_{method}.csv\"\n",
        "                prob_dir = os.path.join(output_dir, \"probs\")\n",
        "                os.makedirs(prob_dir, exist_ok=True)\n",
        "                prob_path = f\"{prob_dir}/{prob_filename}\"\n",
        "                prob_df.to_csv(prob_path, index=False)\n",
        "                print(f\"Saved label probabilities for method: {method}\")\n",
        "\n",
        "    def automatic_tta_submission(self, test_loader, output_dir=\"submissions\"):\n",
        "        predictions, probs, dummy_target = self.tta_predict(test_loader)\n",
        "        self.save_predictions(predictions, probs)\n",
        "\n",
        "    def analyze_predictions(\n",
        "        self, preds, targets, method=\"no_tta\", chart_dir=\"charts/data_orig\"\n",
        "    ):\n",
        "        os.makedirs(chart_dir, exist_ok=True)\n",
        "        preds = preds.clone().detach()\n",
        "        targets = targets.clone().detach()\n",
        "        accuracy_per_class = []\n",
        "        for class_idx in range(self.config.num_classes):\n",
        "            class_mask = targets == class_idx\n",
        "            if class_mask.sum() > 0:\n",
        "                class_accuracy = (\n",
        "                    (preds[class_mask] == targets[class_mask]).float().mean().item()\n",
        "                )\n",
        "            else:\n",
        "                class_accuracy = 0.0\n",
        "            accuracy_per_class.append(class_accuracy)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(range(len(accuracy_per_class)), accuracy_per_class)\n",
        "        plt.title(f\"Accuracy per Class - {method}\")\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        try:\n",
        "            plt.xticks(\n",
        "                range(len(self.config.class_names)),\n",
        "                self.config.class_names,\n",
        "                rotation=90,\n",
        "            )\n",
        "        except:\n",
        "            plt.xticks(range(self.config.num_classes))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(chart_dir, f\"accuracy_per_class_{method}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        conf_matrix = confusion_matrix(targets, preds)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "        plt.title(f\"Confusion Matrix - {method}\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        try:\n",
        "            plt.xticks(\n",
        "                range(len(self.config.class_names)),\n",
        "                self.config.class_names,\n",
        "                rotation=90,\n",
        "            )\n",
        "            plt.yticks(\n",
        "                range(len(self.config.class_names)), self.config.class_names, rotation=0\n",
        "            )\n",
        "        except:\n",
        "            pass\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(chart_dir, f\"confusion_matrix_{method}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        return accuracy_per_class\n",
        "\n",
        "    def analyze_tta(self, val_loader):\n",
        "        results, targets = self.tta_evaluate(val_loader)\n",
        "        targets_tensor = torch.as_tensor(targets)\n",
        "\n",
        "        accuracy_per_class_all = {}\n",
        "\n",
        "        # 차트 저장 경로 설정\n",
        "        try:\n",
        "            disagreement_dir = f\"disagreement_samples/{self.config.group_name}\"\n",
        "            chart_dir = f\"charts/{self.config.group_name}\"\n",
        "        except AttributeError:\n",
        "            chart_dir = os.path.join(\"charts\", \"data_orig\")\n",
        "            disagreement_dir = os.path.join(\"disagreement_samples\", \"data_orig\")\n",
        "        os.makedirs(disagreement_dir, exist_ok=True)\n",
        "        os.makedirs(chart_dir, exist_ok=True)\n",
        "\n",
        "        for method in results.keys():\n",
        "            print(f\"Summary Results for the aggregate method: {method}\")\n",
        "            accuracy_per_class = self.analyze_predictions(\n",
        "                results[method][\"preds\"], targets, method, chart_dir\n",
        "            )\n",
        "            accuracy_per_class_all[method] = accuracy_per_class\n",
        "\n",
        "        # Find images with disagreement\n",
        "        disagreement_mask = torch.zeros(\n",
        "            len(targets_tensor), dtype=torch.bool, device=targets_tensor.device\n",
        "        )\n",
        "        for method in results.keys():\n",
        "            if method != \"ensemble\":\n",
        "                preds_tensor = torch.as_tensor(\n",
        "                    results[method][\"preds\"], device=targets_tensor.device\n",
        "                )\n",
        "                disagreement_mask |= preds_tensor != targets_tensor\n",
        "\n",
        "        disagreement_indices = disagreement_mask.nonzero().squeeze()\n",
        "\n",
        "        # Count disagreements per class\n",
        "        disagreement_counts = torch.bincount(\n",
        "            targets.clone().detach()[disagreement_indices],\n",
        "            minlength=self.config.num_classes,\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(range(self.config.num_classes), disagreement_counts)\n",
        "        plt.title(\"Disagreements per Class\")\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Number of Disagreements\")\n",
        "        try:\n",
        "            plt.xticks(\n",
        "                range(len(self.config.class_names)),\n",
        "                self.config.class_names,\n",
        "                rotation=90,\n",
        "            )\n",
        "        except:\n",
        "            plt.xticks(range(self.config.num_classes))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(chart_dir, \"disagreements_per_class.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Save disagreement images\n",
        "        dataset = val_loader.dataset\n",
        "        for idx in disagreement_indices[\n",
        "            : min(len(disagreement_indices), 100)\n",
        "        ]:  # Save up to 100 samples\n",
        "            try:\n",
        "                img, _ = dataset[idx]\n",
        "                img_np = img.permute(1, 2, 0).numpy()\n",
        "                img_np = (img_np - img_np.min()) / (\n",
        "                    img_np.max() - img_np.min()\n",
        "                )  # Normalize to [0, 1]\n",
        "                img_path = os.path.join(\n",
        "                    disagreement_dir, f\"disagreement_sample_{idx}.png\"\n",
        "                )\n",
        "                plt.imsave(img_path, img_np)\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving disagreement sample {idx}: {str(e)}\")\n",
        "        print(f\"Saved disagreement samples to {disagreement_dir}\")\n",
        "\n",
        "        # Create a new dataset with only disagreement samples\n",
        "        disagreement_dataset = torch.utils.data.Subset(dataset, disagreement_indices)\n",
        "\n",
        "        # Create DataFrame for accuracy per class\n",
        "        try:\n",
        "            class_names = self.config.class_names\n",
        "        except:\n",
        "            class_names = [f\"Class {i}\" for i in range(self.config.num_classes)]\n",
        "        df_accuracy = pd.DataFrame(accuracy_per_class_all).T\n",
        "        df_accuracy.columns = class_names\n",
        "\n",
        "        # Add average accuracy row\n",
        "        df_accuracy.loc[\"Average\"] = df_accuracy.mean()\n",
        "\n",
        "        # Round all values to 4 decimal places\n",
        "        df_accuracy = df_accuracy.round(4)\n",
        "\n",
        "        print(\"\\nAccuracy per class for all methods:\")\n",
        "        display(df_accuracy)\n",
        "        # Save the dataframe as an image\n",
        "        self._save_dataframe_as_image(\n",
        "            df_accuracy, os.path.join(chart_dir, \"accuracy_per_class_all_methods.png\")\n",
        "        )\n",
        "\n",
        "        return disagreement_dataset, df_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mixup & Cutmix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1.0 - lam)\n",
        "    cut_w = int(W * cut_rat)  # np.int() 대신 int() 사용\n",
        "    cut_h = int(H * cut_rat)  # np.int() 대신 int() 사용\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    y_a, y_b = y, y[index]\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
        "\n",
        "    return x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "커스텀 모델 -> 수정, 보완 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.config = config\n",
        "        self.backbone = timm.create_model(\n",
        "            config.model_name, pretrained=config.pretrained, num_classes=0, in_chans=3\n",
        "        )\n",
        "\n",
        "        if hasattr(self.backbone, \"num_features\"):\n",
        "            num_features = self.backbone.num_features\n",
        "        elif hasattr(self.backbone, \"fc\"):\n",
        "            num_features = self.backbone.fc.in_features\n",
        "        else:\n",
        "            raise ValueError(\"Unable to determine number of features in backbone\")\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, config.num_classes),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.metrics = CustomMetrics(num_classes=config.num_classes).to(config.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        out = self.classifier(features)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, loss_func):\n",
        "        x, y = batch\n",
        "        rand_num = np.random.rand()\n",
        "        if rand_num < self.config.mixup_prob:\n",
        "            mixed_x, y_a, y_b, lam = mixup_data(x, y, self.config.mixup_alpha)\n",
        "            logits = self(mixed_x)\n",
        "            loss = mixup_criterion(loss_func, logits, y_a, y_b, lam)\n",
        "        elif rand_num < self.config.mixup_prob + self.config.cutmix_prob:\n",
        "            mixed_x, y_a, y_b, lam = cutmix_data(x, y, self.config.cutmix_alpha)\n",
        "            logits = self(mixed_x)\n",
        "            loss = cutmix_criterion(loss_func, logits, y_a, y_b, lam)\n",
        "        else:\n",
        "            logits = self(x)\n",
        "            loss = loss_func(logits, y)\n",
        "\n",
        "        metrics = self.metrics(logits, y)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def validation_step(self, batch, loss_func):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = loss_func(logits, y)\n",
        "\n",
        "        metrics = self.metrics(logits, y)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        x, img_names = batch\n",
        "        logits = self(x)\n",
        "        prob = self.softmax(logits)\n",
        "        return prob, img_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'caformer_m36.sail_in22k_ft_in1k'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 백본 확인\n",
        "config.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unexpected keys (head.fc.fc1.bias, head.fc.fc1.weight, head.fc.norm.bias, head.fc.norm.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        }
      ],
      "source": [
        "# 모델 준비\n",
        "model = CustomModel(config).to(config.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "손실함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmoothFocalLoss(nn.Module):\n",
        "    def __init__(self, num_classes, alpha=None, gamma=2, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.gamma = gamma\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "        if alpha is None:\n",
        "            self.alpha = torch.ones(num_classes)\n",
        "        else:\n",
        "            self.alpha = torch.tensor(alpha)\n",
        "\n",
        "        self.alpha = self.alpha / self.alpha.sum()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "\n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "\n",
        "        # 라벨 스무딩 적용\n",
        "        targets_smooth = (\n",
        "            1 - self.smoothing\n",
        "        ) * targets + self.smoothing / self.num_classes\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -targets_smooth * log_probs\n",
        "\n",
        "        pt = torch.exp(-loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha.to(inputs.device)\n",
        "            focal_loss = alpha.unsqueeze(0) * focal_loss\n",
        "\n",
        "        return focal_loss.sum(dim=1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, num_classes, alpha=None, gamma=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.gamma = gamma\n",
        "\n",
        "        if alpha is None:\n",
        "            self.alpha = torch.ones(num_classes)\n",
        "        else:\n",
        "            self.alpha = torch.tensor(alpha)\n",
        "\n",
        "        self.alpha = self.alpha / self.alpha.sum()\n",
        "\n",
        "    def __call__(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "\n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha.to(inputs.device)\n",
        "            focal_loss = alpha.unsqueeze(0) * focal_loss\n",
        "\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CELoss(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __call__(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "\n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
        "\n",
        "        return ce_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmoothCELoss(nn.Module):\n",
        "    def __init__(self, num_classes, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def __call__(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "\n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "\n",
        "        # 라벨 스무딩 적용\n",
        "        targets_smooth = (\n",
        "            1 - self.smoothing\n",
        "        ) * targets + self.smoothing / self.num_classes\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -targets_smooth * log_probs\n",
        "\n",
        "        return loss.sum(dim=1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_loss_function(config):\n",
        "    if config.loss == \"ce\":\n",
        "        return CELoss(num_classes=config.num_classes)\n",
        "    elif config.loss == \"focal\":\n",
        "        return FocalLoss(\n",
        "            num_classes=config.num_classes,\n",
        "            alpha=config.focal_alpha,\n",
        "            gamma=config.focal_gamma,\n",
        "        )\n",
        "    elif config.loss == \"smoothce\":\n",
        "        return SmoothCELoss(num_classes=config.num_classes, smoothing=config.smoothing)\n",
        "    elif config.loss == \"smoothfocal\":\n",
        "        return SmoothFocalLoss(\n",
        "            num_classes=config.num_classes,\n",
        "            alpha=config.focal_alpha,\n",
        "            gamma=config.focal_gamma,\n",
        "            smoothing=config.smoothing,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported loss function: {config.loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "옵티마이저"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Ranger(optim.Optimizer):  # from torch.optim.optimizer import Optimizer\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr=1e-3,  # lr\n",
        "        alpha=0.5,\n",
        "        k=6,\n",
        "        N_sma_threshhold=5,  # Ranger options\n",
        "        betas=(0.95, 0.999),\n",
        "        eps=1e-5,\n",
        "        weight_decay=0,  # Adam options\n",
        "        # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
        "        use_gc=True,\n",
        "        gc_conv_only=False,\n",
        "    ):\n",
        "\n",
        "        # parameter checks\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f\"Invalid lookahead steps: {k}\")\n",
        "        if not lr > 0:\n",
        "            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n",
        "        if not eps > 0:\n",
        "            raise ValueError(f\"Invalid eps: {eps}\")\n",
        "\n",
        "        # prep defaults and init torch.optim base\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            alpha=alpha,\n",
        "            k=k,\n",
        "            step_counter=0,\n",
        "            betas=betas,\n",
        "            N_sma_threshhold=N_sma_threshhold,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        # adjustable threshold\n",
        "        self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "        # look ahead params\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "\n",
        "        # radam buffer for state\n",
        "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
        "\n",
        "        # gc on or off\n",
        "        self.use_gc = use_gc\n",
        "\n",
        "        # level of gradient centralization\n",
        "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
        "\n",
        "        print(\n",
        "            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\"\n",
        "        )\n",
        "        if self.use_gc and self.gc_gradient_threshold == 1:\n",
        "            print(f\"GC applied to both conv and fc layers\")\n",
        "        elif self.use_gc and self.gc_gradient_threshold == 3:\n",
        "            print(f\"GC applied to conv layers only\")\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        print(\"set state called\")\n",
        "        super(Ranger, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "\n",
        "        # Evaluate averages and grad, update param tensors\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        \"Ranger optimizer does not support sparse gradients\"\n",
        "                    )\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]  # get state dict for this param\n",
        "\n",
        "                if len(state) == 0:\n",
        "\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                    # look ahead weight storage now in state dict\n",
        "                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n",
        "                    state[\"slow_buffer\"].copy_(p.data)\n",
        "\n",
        "                else:\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
        "\n",
        "                # begin computations\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                # GC operation for Conv layers and FC layers\n",
        "                if grad.dim() > self.gc_gradient_threshold:\n",
        "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                # compute variance mov avg\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                # compute mean moving avg\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n",
        "\n",
        "                if state[\"step\"] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state[\"step\"]\n",
        "                    beta2_t = beta2 ** state[\"step\"]\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > self.N_sma_threshhold:\n",
        "                        step_size = math.sqrt(\n",
        "                            (1 - beta2_t)\n",
        "                            * (N_sma - 4)\n",
        "                            / (N_sma_max - 4)\n",
        "                            * (N_sma - 2)\n",
        "                            / N_sma\n",
        "                            * N_sma_max\n",
        "                            / (N_sma_max - 2)\n",
        "                        ) / (1 - beta1 ** state[\"step\"])\n",
        "                    else:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "\n",
        "                # apply lr\n",
        "                if N_sma > self.N_sma_threshhold:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "                # integrated look ahead...\n",
        "                # we do it at the param level instead of group level\n",
        "                if state[\"step\"] % group[\"k\"] == 0:\n",
        "                    # get access to slow param tensor\n",
        "                    slow_p = state[\"slow_buffer\"]\n",
        "                    # (fast weights - slow weights) * alpha\n",
        "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
        "                    # copy interpolated weights to RAdam param tensor\n",
        "                    p.data.copy_(slow_p)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimizer(model_params, config):\n",
        "    if config.optimizer == \"SGD\":\n",
        "        return optim.SGD(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            momentum=config.momentum,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "    elif config.optimizer == \"Adam\":\n",
        "        return optim.Adam(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            betas=(config.adam_beta1, config.adam_beta2),\n",
        "            eps=config.adam_epsilon,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "    elif config.optimizer == \"AdamW\":\n",
        "        return optim.AdamW(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            betas=(config.adam_beta1, config.adam_beta2),\n",
        "            eps=config.adam_epsilon,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "    elif config.optimizer == \"RMSprop\":\n",
        "        return optim.RMSprop(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            alpha=config.rmsprop_alpha,\n",
        "            eps=config.rmsprop_epsilon,\n",
        "            weight_decay=config.weight_decay,\n",
        "            momentum=config.momentum,\n",
        "        )\n",
        "    elif config.optimizer == \"Adadelta\":\n",
        "        return optim.Adadelta(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            rho=config.adadelta_rho,\n",
        "            eps=config.adadelta_epsilon,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "    elif config.optimizer == \"Ranger\":\n",
        "        return Ranger(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            alpha=config.ranger_alpha,\n",
        "            k=config.ranger_k,\n",
        "            N_sma_threshhold=config.ranger_N_sma_threshhold,\n",
        "            betas=(config.ranger_beta1, config.ranger_beta2),\n",
        "            eps=config.ranger_eps,\n",
        "            weight_decay=config.weight_decay,\n",
        "            use_gc=config.ranger_use_gc,\n",
        "            gc_conv_only=config.ranger_gc_conv_only,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {config.optimizer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "스케쥴러"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_scheduler(optimizer, config):\n",
        "    if config.scheduler == \"LambdaLR\":\n",
        "        return optim.lr_scheduler.LambdaLR(\n",
        "            optimizer=optimizer,\n",
        "            lr_lambda=lambda epoch: config.lambda_factor**epoch,\n",
        "            last_epoch=config.last_epoch,\n",
        "            verbose=config.verbose,\n",
        "        )\n",
        "    elif config.scheduler == \"MultiplicativeLR\":\n",
        "        return optim.lr_scheduler.MultiplicativeLR(\n",
        "            optimizer=optimizer, lr_lambda=lambda epoch: config.lambda_factor**epoch\n",
        "        )\n",
        "    elif config.scheduler == \"StepLR\":\n",
        "        return optim.lr_scheduler.StepLR(\n",
        "            optimizer, step_size=config.step_size, gamma=config.gamma\n",
        "        )\n",
        "    elif config.scheduler == \"CosineAnnealingLR\":\n",
        "        return optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=config.T_max, eta_min=config.eta_min\n",
        "        )\n",
        "    elif config.scheduler == \"ReduceLROnPlateau\":\n",
        "        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=config.mode)\n",
        "    elif config.scheduler == \"CyclicLR\":\n",
        "        return optim.lr_scheduler.CyclicLR(\n",
        "            optimizer,\n",
        "            base_lr=config.base_lr,\n",
        "            max_lr=config.max_lr,\n",
        "            step_size_up=config.step_size_up,\n",
        "            mode=config.mode,\n",
        "            gamma=config.gamma,\n",
        "        )\n",
        "    elif config.scheduler == \"OneCycleLR\":\n",
        "        return optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=config.max_lr,\n",
        "            steps_per_epoch=config.steps_per_epoch,\n",
        "            epochs=config.epochs,\n",
        "            anneal_strategy=config.anneal_strategy,\n",
        "        )\n",
        "    elif config.scheduler == \"CosineAnnealingWarmRestarts\":\n",
        "        return optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=config.T_0, T_mult=config.T_mult, eta_min=config.eta_min\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {config.scheduler}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(SmoothFocalLoss(),\n",
              " Ranger (\n",
              " Parameter Group 0\n",
              "     N_sma_threshhold: 5\n",
              "     alpha: 0.5\n",
              "     betas: (0.95, 0.999)\n",
              "     eps: 1e-05\n",
              "     initial_lr: 0.001\n",
              "     k: 6\n",
              "     lr: 0.001\n",
              "     step_counter: 0\n",
              "     weight_decay: 1e-05\n",
              " ),\n",
              " <torch.optim.lr_scheduler.CosineAnnealingWarmRestarts at 0x7f9de1f97910>)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 손실함수, 옵티마이저, 스케줄러 준비\n",
        "loss_func = get_loss_function(config)\n",
        "optimizer_choice = get_optimizer(model.parameters(), config)\n",
        "scheduler = get_scheduler(optimizer_choice, config)\n",
        "loss_func, optimizer_choice, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "훈련 - 트레이너"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.group_name = self.config.group_name\n",
        "        self.save_config()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.best_val_f1 = 0\n",
        "        self.start_epoch = 0\n",
        "        self.total_epochs = self.config.epochs\n",
        "        self.loss_func = self.get_loss_function()\n",
        "        self.optimizer = self.get_optimizer()\n",
        "        self.scheduler = self.get_scheduler()\n",
        "        self.scaler = GradScaler(enabled=config.use_mixed_precision)\n",
        "        self.early_stopping_counter = 0\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "        self.metrics = CustomMetrics(num_classes=config.num_classes).to(self.device)\n",
        "\n",
        "        self.use_mixed_precision = config.use_mixed_precision\n",
        "        if self.use_mixed_precision:\n",
        "            self.scaler = GradScaler()\n",
        "\n",
        "    #################### 기본 세팅 ####################\n",
        "    def initialize_model(self):\n",
        "        return CustomModel(self.config).to(self.config.device)\n",
        "\n",
        "    def get_loss_function(self):\n",
        "        return get_loss_function(self.config)\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return get_optimizer(self.model.parameters(), self.config)\n",
        "\n",
        "    def get_scheduler(self):\n",
        "        return get_scheduler(self.optimizer, self.config)\n",
        "\n",
        "    ##########################################################\n",
        "    @staticmethod\n",
        "    def get_short_model_name(model_name):\n",
        "        return model_name.split(\".\")[0]\n",
        "\n",
        "    def generate_new_group_name(self, old_group_name=False):\n",
        "        if old_group_name:\n",
        "            parts = old_group_name.rsplit(\"_\", 1)\n",
        "            new_suffix = wandb.util.generate_id()[:8]\n",
        "            return f\"{parts[0]}_{new_suffix}\"\n",
        "        else:\n",
        "            model_short_name = self.get_short_model_name(self.config.model_name)\n",
        "            return f\"{model_short_name}_{self.config.optimizer}_{wandb.util.generate_id()[:8]}\"\n",
        "\n",
        "    ################세이브 로드 ####################\n",
        "    def save_checkpoint(self, epoch, val_f1):\n",
        "        checkpoint_path = f\"models/{self.group_name}/checkpoint_epoch_{epoch+1}.pt\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": self.model.state_dict(),\n",
        "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
        "                \"config\": dict(self.config),\n",
        "                \"val_f1_macro\": val_f1,\n",
        "            },\n",
        "            checkpoint_path,\n",
        "        )\n",
        "        if val_f1 > self.best_val_f1:\n",
        "            self.best_val_f1 = val_f1\n",
        "            best_model_path = f\"models/{self.group_name}/best_model.pt\"\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": self.model.state_dict(),\n",
        "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                    \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
        "                    \"config\": dict(self.config),\n",
        "                    \"val_f1_macro\": val_f1,\n",
        "                },\n",
        "                best_model_path,\n",
        "            )\n",
        "\n",
        "    def load_model(self, path):\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Model file not found: {path}\")\n",
        "            return False\n",
        "        try:\n",
        "            checkpoint = torch.load(path, map_location=self.device)\n",
        "            self.config.__dict__.update(checkpoint[\"config\"])\n",
        "            self.model = CustomModel(self.config).to(self.device)\n",
        "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "            self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "            self.start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            self.best_val_f1 = checkpoint[\"val_f1_macro\"]\n",
        "            self.group_name = self.config.group_name\n",
        "            print(f\"Model loaded from {path}\")\n",
        "            print(f\"Continuing training from epoch {self.start_epoch}\")\n",
        "            print(f\"Best validation F1 score: {self.best_val_f1:.4f}\")\n",
        "            self.model.train()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def load_best_model(self, group_name=None):\n",
        "        if group_name is None:\n",
        "            group_name = self.group_name\n",
        "        best_model_path = os.path.join(\"models\", group_name, \"best_model.pt\")\n",
        "        if not os.path.exists(best_model_path):\n",
        "            print(f\"No best model found for group {group_name}\")\n",
        "            return False\n",
        "        return self.load_model(best_model_path)\n",
        "\n",
        "    ######################## Config 저장 #######################\n",
        "    def save_config(self):\n",
        "        config_path = f\"models/{self.group_name}/config.json\"\n",
        "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "        with open(config_path, \"w\") as f:\n",
        "            json.dump(dict(self.config), f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_config(group_name):\n",
        "        config_path = f\"models/{group_name}/config.json\"\n",
        "        with open(config_path, \"r\") as f:\n",
        "            config_dict = json.load(f)\n",
        "        return Config(**config_dict)\n",
        "\n",
        "    #################### 훈련 스크립트 ####################\n",
        "    def train_epoch_mixed_precision(self, train_loader, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        metrics = self.initialize_metrics()\n",
        "        pbar = tqdm(\n",
        "            train_loader, desc=f\"Epoch {epoch+1}/{self.total_epochs}\", leave=False\n",
        "        )\n",
        "        for batch in pbar:\n",
        "            x, y = batch\n",
        "            with autocast():\n",
        "                outputs = self.model(x)\n",
        "                loss = self.loss_func(outputs, y)\n",
        "            self.scaler.scale(loss).backward()\n",
        "            if self.config.gradient_clip_val > 0:\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(), self.config.gradient_clip_val\n",
        "                )\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "            self.optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            self.update_metrics(metrics, {\"preds\": preds, \"targets\": y})\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        return total_loss / len(train_loader), self.calculate_metrics(metrics)\n",
        "\n",
        "    def train_epoch_full_precision(self, train_loader, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        metrics = self.initialize_metrics()\n",
        "        pbar = tqdm(\n",
        "            train_loader, desc=f\"Epoch {epoch+1}/{self.total_epochs}\", leave=False\n",
        "        )\n",
        "        for batch in pbar:\n",
        "            x, y = batch\n",
        "            outputs = self.model(x)\n",
        "            loss = self.loss_func(outputs, y)\n",
        "            loss.backward()\n",
        "            if self.config.gradient_clip_val > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(), self.config.gradient_clip_val\n",
        "                )\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            self.update_metrics(metrics, {\"preds\": preds, \"targets\": y})\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        return total_loss / len(train_loader), self.calculate_metrics(metrics)\n",
        "\n",
        "    def train(self, train_loader, validation_loader, is_validate=False):\n",
        "        wandb.init(\n",
        "            project=self.config.project_name,\n",
        "            config=vars(self.config),\n",
        "            group=self.group_name,\n",
        "            job_type=\"new_run\",\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(self.start_epoch, self.total_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            if self.use_mixed_precision:\n",
        "                train_loss, train_metrics = self.train_epoch_mixed_precision(\n",
        "                    train_loader, epoch\n",
        "                )\n",
        "            else:\n",
        "                train_loss, train_metrics = self.train_epoch_full_precision(\n",
        "                    train_loader, epoch\n",
        "                )\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            self.log_metrics(epoch, train_loss, train_metrics)\n",
        "            self.update_scheduler(train_loss)\n",
        "            self.save_checkpoint(epoch, train_metrics[\"f1_macro\"])\n",
        "            print(\n",
        "                f\"Epoch {epoch+1}/{self.total_epochs} - \"\n",
        "                f\"Loss: {train_loss:.4f}, F1 Macro: {train_metrics['f1_macro']:.4f}, \"\n",
        "                f\"Lowest_class_accuracy: {train_metrics['lowest_class_accuracy']:.4f}, Time: {epoch_time:.2f}s\"\n",
        "            )\n",
        "            if self.early_stopping(train_loss):\n",
        "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "                break\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Training completed. Total time: {total_time:.2f}s\")\n",
        "        return train_loss, train_metrics\n",
        "\n",
        "    def validate(self, val_loader, epoch_pbar):\n",
        "        self.model.eval()\n",
        "        total_loss, total_metrics = 0, self.initialize_metrics()\n",
        "\n",
        "        batch_pbar = tqdm(val_loader, desc=\"Validation\", position=1, leave=False)\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                batch = [b.to(self.device) for b in batch]\n",
        "                if self.config.use_mixed_precision:\n",
        "                    with autocast():\n",
        "                        loss, metrics = self.model.validation_step(\n",
        "                            batch, self.loss_func\n",
        "                        )\n",
        "                else:\n",
        "                    loss, metrics = self.model.validation_step(batch, self.loss_func)\n",
        "                total_loss += loss.item()\n",
        "                self.update_metrics(total_metrics, metrics)\n",
        "\n",
        "                # Update epoch progress bar with batch information\n",
        "                epoch_pbar.set_description(\n",
        "                    f\"Epoch {epoch_pbar.n+1} - Val batch {i+1}/{len(val_loader)}\"\n",
        "                )\n",
        "\n",
        "        return total_loss / len(val_loader), self.finalize_metrics(\n",
        "            total_metrics, len(val_loader)\n",
        "        )\n",
        "\n",
        "    def setup_training(\n",
        "        self, checkpoint_path=None, continue_training=False, additional_epochs=0\n",
        "    ):\n",
        "        if checkpoint_path:\n",
        "            self.load_model(checkpoint_path)\n",
        "        else:\n",
        "            self.load_model(checkpoint_path)\n",
        "            self.start_epoch = 0\n",
        "            self.best_val_f1 = 0\n",
        "            self.group_name = self.generate_new_group_name()\n",
        "\n",
        "        if continue_training:\n",
        "            self.group_name = self.generate_new_group_name(self.group_name)\n",
        "            print(f\"Continuing training with new group_name: {self.group_name}\")\n",
        "            self.total_epochs = self.start_epoch + additional_epochs\n",
        "        else:\n",
        "            self.total_epochs = self.config.epochs\n",
        "\n",
        "        self.config.group_name = self.group_name\n",
        "        self.model.train()\n",
        "        self.scaler = GradScaler(enabled=self.config.use_mixed_precision)\n",
        "\n",
        "        if continue_training:\n",
        "            self.optimizer = self.get_optimizer()\n",
        "            self.scheduler = self.get_scheduler()\n",
        "\n",
        "    #################### 기록관련 ####################\n",
        "    def initialize_metrics(self):\n",
        "        return {\"preds\": [], \"targets\": []}\n",
        "\n",
        "    def update_metrics(self, metrics, batch_metrics):\n",
        "        metrics[\"preds\"].extend(batch_metrics[\"preds\"].cpu().numpy())\n",
        "        metrics[\"targets\"].extend(batch_metrics[\"targets\"].cpu().numpy())\n",
        "\n",
        "    def calculate_metrics(self, metrics):\n",
        "        preds = torch.from_numpy(np.array(metrics[\"preds\"])).to(self.device)\n",
        "        targets = torch.from_numpy(np.array(metrics[\"targets\"])).to(self.device)\n",
        "        return self.metrics(preds, targets)\n",
        "\n",
        "    def log_metrics(self, epoch, train_loss, train_metrics):\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_f1_macro\": train_metrics[\"f1_macro\"],\n",
        "                \"train_lowest_class_accuracy\": train_metrics[\"lowest_class_accuracy\"],\n",
        "                \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "            }\n",
        "        )\n",
        "        for i, acc in enumerate(train_metrics[\"accuracies_per_class\"]):\n",
        "            wandb.log({f\"train_class_{i}_accuracy\": acc.item()})\n",
        "\n",
        "    #################### 훈련 관련 ####################\n",
        "    def warmup_learning_rate(self, epoch):\n",
        "        lr = (\n",
        "            self.config.learning_rate\n",
        "            * ((epoch + 1) / self.config.warmup_epochs)\n",
        "            * self.config.warmup_factor\n",
        "        )\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    def update_scheduler(self, val_loss):\n",
        "        if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            self.scheduler.step(val_loss)\n",
        "        else:\n",
        "            self.scheduler.step()\n",
        "\n",
        "    def early_stopping(self, val_loss):\n",
        "        if val_loss < self.best_val_loss - self.config.early_stopping_delta:\n",
        "            self.best_val_loss = val_loss\n",
        "            self.early_stopping_counter = 0\n",
        "        else:\n",
        "            self.early_stopping_counter += 1\n",
        "            if self.early_stopping_counter >= self.config.early_stopping_patience:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "디버깅 (작은 데이터셋 준비)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1004"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "int(0.01 * len(train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size2 = int(0.01 * len(train_ds))\n",
        "val_size2 = int(0.01 * len(train_ds))\n",
        "rem_size2 = len(train_ds) - train_size2 - val_size2\n",
        "train_ds2, val_ds2, rem_ds2 = random_split(\n",
        "    train_ds, [train_size2, val_size2, rem_size2]\n",
        ")\n",
        "# DataLoader 정의\n",
        "train_loader2 = DataLoader(\n",
        "    train_ds2,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "val_loader2 = DataLoader(\n",
        "    val_ds2,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_device(x, device):\n",
        "    return x.to(device) if isinstance(x, torch.Tensor) else x\n",
        "\n",
        "\n",
        "class DeviceDataLoader:\n",
        "    def __init__(self, dataloader, device):\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.dataloader:\n",
        "            yield tuple(to_device(x, self.device) for x in batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "\n",
        "\n",
        "# 데이터 로더를 GPU로 이동\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader2 = DeviceDataLoader(val_loader2, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocated: 0.40 GB\n",
            "Cached: 0.44 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def print_gpu_memory():\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:9nz16755) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24845d680d7f42bab324bf360fe814fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">distinctive-terrain-229</strong> at: <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/9nz16755' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/9nz16755</a><br/> View project at: <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240805_192526-9nz16755/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:9nz16755). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/ephemeral/home/testbed/wandb/run-20240805_230119-95z4g3e7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/95z4g3e7' target=\"_blank\">stilted-field-234</a></strong> to <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/95z4g3e7' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/95z4g3e7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/300:   0%|          | 0/25120 [00:00<?, ?it/s]/tmp/ipykernel_937343/2979457354.py:98: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
            "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 - Loss: 0.0182, F1 Macro: 0.7606, Lowest_class_accuracy: 0.2205, Time: 2768.74s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/300 - Loss: 0.0074, F1 Macro: 0.8730, Lowest_class_accuracy: 0.4042, Time: 2773.70s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/300 - Loss: 0.0052, F1 Macro: 0.9056, Lowest_class_accuracy: 0.5240, Time: 2777.11s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/300 - Loss: 0.0036, F1 Macro: 0.9278, Lowest_class_accuracy: 0.6037, Time: 2775.75s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/300 - Loss: 0.0025, F1 Macro: 0.9469, Lowest_class_accuracy: 0.6783, Time: 2772.80s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/300 - Loss: 0.0015, F1 Macro: 0.9679, Lowest_class_accuracy: 0.7888, Time: 2774.34s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/300 - Loss: 0.0009, F1 Macro: 0.9831, Lowest_class_accuracy: 0.8856, Time: 2771.40s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/300 - Loss: 0.0004, F1 Macro: 0.9923, Lowest_class_accuracy: 0.9410, Time: 2770.95s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/300 - Loss: 0.0002, F1 Macro: 0.9973, Lowest_class_accuracy: 0.9792, Time: 2767.46s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/300 - Loss: 0.0001, F1 Macro: 0.9992, Lowest_class_accuracy: 0.9930, Time: 2769.53s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/300 - Loss: nan, F1 Macro: 0.9412, Lowest_class_accuracy: 0.7078, Time: 2770.01s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/300 - Loss: 0.0035, F1 Macro: 0.9448, Lowest_class_accuracy: 0.7150, Time: 2782.31s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/300 - Loss: 0.0027, F1 Macro: 0.9548, Lowest_class_accuracy: 0.7525, Time: 2785.08s\n",
            "Early stopping triggered after epoch 13\n",
            "Training completed. Total time: 36088.20s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.002734373151386848,\n",
              " {'f1_macro': tensor(0.9548, device='cuda:0'),\n",
              "  'lowest_class_accuracy': tensor(0.7525, device='cuda:0'),\n",
              "  'accuracies_per_class': tensor([0.9967, 0.9817, 0.9964, 0.7890, 0.9154, 0.9950, 0.9886, 0.7525, 0.9939,\n",
              "          0.9978, 0.9899, 0.9761, 0.9820, 0.9723, 0.9384, 0.9952, 0.9936],\n",
              "         device='cuda:0')})"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 일반 훈련\n",
        "trainer = Trainer(model, config)\n",
        "trainer.train(train_loader, val_loader2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 체크포인트, 연속 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(model, config)  # 아무거나 불러와도 됨\n",
        "trainer.setup_training(\n",
        "    checkpoint_path=\"models/tiny_vit_11m_224_Ranger_hzeqx347/checkpoint_epoch_36.pt\",\n",
        "    continue_training=False,\n",
        ")\n",
        "trainer.train(train_loader, validation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 체크포인트에서 훈련 계속 (10 에폭 추가)\n",
        "trainer = Trainer(model, config)\n",
        "trainer.setup_training(\n",
        "    checkpoint_path=\"models/tiny_vit_11m_224_Ranger_hzeqx347/checkpoint_epoch_36.pt\",\n",
        "    continue_training=True,\n",
        "    additional_epochs=10,\n",
        ")\n",
        "trainer.train(train_loader, validation_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 분석 및 결과제출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unexpected keys (head.fc.fc1.bias, head.fc.fc1.weight, head.fc.norm.bias, head.fc.norm.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set state called\n",
            "Model loaded from models/caformer_m36_Ranger_1zo3gt64/checkpoint_epoch_10.pt\n",
            "Continuing training from epoch 10\n",
            "Best validation F1 score: 0.9992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 785/785 [05:36<00:00,  2.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions for method: no_tta\n",
            "Saved label probabilities for method: no_tta\n",
            "Saved predictions for method: mean\n",
            "Saved label probabilities for method: mean\n",
            "Saved predictions for method: max\n",
            "Saved label probabilities for method: max\n",
            "Saved predictions for method: temp_sharpen\n",
            "Saved label probabilities for method: temp_sharpen\n",
            "Saved predictions for method: mode\n",
            "Saved label probabilities for method: mode\n",
            "Saved predictions for method: modethreshold\n",
            "Saved label probabilities for method: modethreshold\n",
            "Saved predictions for method: ensemble\n"
          ]
        }
      ],
      "source": [
        "# 분석 예시\n",
        "trainer = Trainer(model, config)  # 아무거나 시작, 사용한 모델과 컨피그는 아무거나\n",
        "# trainer.load_best_model(group_name='tiny_vit_11m_224_Ranger_4j1cx18a')\n",
        "trainer.load_model(\"models/caformer_m36_Ranger_1zo3gt64/checkpoint_epoch_10.pt\")\n",
        "tta = TTA(trainer.model, trainer.config)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 테스트 데이터에 대한 예측 및 제출 파일 생성\n",
        "tta.automatic_tta_submission(test_loader, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions:  16%|█▋        | 4093/25120 [29:23<2:27:46,  2.37it/s]"
          ]
        }
      ],
      "source": [
        "# 검증 데이터에 대한 TTA 분석\n",
        "disagreement_dataset = tta.analyze_tta(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Focal Alpha 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Focal Weight 업데이트\n",
        "error_rates = [\n",
        "    0.115,\n",
        "    0.115,\n",
        "    0.106,\n",
        "    0.021,\n",
        "    0.018,\n",
        "    0.014,\n",
        "    0.011,\n",
        "    0.011,\n",
        "    0.004,\n",
        "    0.004,\n",
        "    0.004,\n",
        "    0.0,\n",
        "    0.0,\n",
        "    0.0,\n",
        "    0.0,\n",
        "    0.0,\n",
        "    0.0,\n",
        "]\n",
        "max_error = max(error_rates)\n",
        "alphas = [min(1.0, (rate / max_error) * 2) for rate in error_rates]\n",
        "alphas = [max(0.1, alpha) for alpha in alphas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 추가 구축 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3,4,7,14\n",
        "오분류한 데이터들 모으기 -> 증강 -> 학습\n",
        "(기존모델 fine tune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SIMCLR -> 자주 틀리는 라벨에 한해서 구현하면 될듯 하다. 그리고 그 라벨로 예측한 것에 2차분류기를 simclr로 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Knowledge Distillation 크고 좋은 모델을 날잡고 훈련해서 knowledge distillation을 시도해봐도 좋을듯 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2차분류기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Progressive Resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adversarial Training (성능 약화 예상)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convolutional Block Attention Module (CBAM) -> 해볼 가치 충분. CNN 기반에 잘 돌아감"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Squeeze-and-Excitation (SE) 블록"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
