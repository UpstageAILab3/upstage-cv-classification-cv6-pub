{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.26.0)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
            "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.16.0)\n",
            "Collecting torchmetrics (from -r requirements.txt (line 10))\n",
            "  Obtaining dependency information for torchmetrics from https://files.pythonhosted.org/packages/29/1b/b38033e61c28e52dde7bd459df6567c04c127ee153722c73b9acd0fe550b/torchmetrics-1.4.1-py3-none-any.whl.metadata\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.0.8)\n",
            "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.17.5)\n",
            "Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (2023.9.2)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 9)) (2.31.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->-r requirements.txt (line 10))\n",
            "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/ea/d5/ed204bc738672c17455019b5e0c7c8d1effb0ea17707150ca50336298ca0/lightning_utilities-0.11.6-py3-none-any.whl.metadata\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (8.0.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (4.1.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (5.27.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (5.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (2.12.0)\n",
            "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 12)) (68.0.0)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 13)) (0.22.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 13)) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 13)) (4.8.1.78)\n",
            "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 9)) (2023.7.22)\n",
            "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 13)) (2.33.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 13)) (2023.12.9)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 13)) (0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 8)) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12)) (5.0.1)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.6 torchmetrics-1.4.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "#pip install pillow numpy pandas seaborn matplotlib tqdm scikit-learn torch torchvision torchmetrics timm wandb albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "라이브러리 목록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "import json\n",
        "import math\n",
        "import timm\n",
        "import time\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import albumentations as album\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchmetrics import F1Score, classification\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <함수 준비> 일괄 실행 가능!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "쿠다 비워내는 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_gpu_memory():\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "커스텀 Config 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Config({', '.join(f'{k}={v}' for k, v in self.__dict__.items())})\"\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.__dict__.items())\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return getattr(self, key)\n",
        "\n",
        "    def get(self, key, default=None):\n",
        "        return getattr(self, key, default)\n",
        "    \n",
        "    def update(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, csv, path, transform=None):\n",
        "        self.df = pd.read_csv(csv).values\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name, target = self.df[idx]\n",
        "        img = np.array(PIL.Image.open(os.path.join(self.path, name)))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "        return img, int(target)  # 타겟을 정수로 반환\n",
        "\n",
        "def get_transform(img_size, mean, std):\n",
        "    return album.Compose([\n",
        "        album.LongestMaxSize(max_size=img_size),\n",
        "        album.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=(0,0,0)),\n",
        "        album.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def create_data_set(base_path, img_size= 448, mean = (0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225)):\n",
        "    # 경로 설정\n",
        "    train_csv = os.path.join(base_path, \"aug_train.csv\")\n",
        "    train_dir = os.path.join(base_path, \"aug_train\")\n",
        "    #train_csv = os.path.join(base_path, \"train.csv\")\n",
        "    #train_dir = os.path.join(base_path, \"train\")\n",
        "\n",
        "    test_csv = os.path.join(base_path, \"true_label.csv\")\n",
        "    test_dir = os.path.join(base_path, \"test\")\n",
        "\n",
        "    # Dataset 정의\n",
        "    trn_dataset = ImageDataset(\n",
        "        train_csv, \n",
        "        train_dir, \n",
        "        transform=get_transform(img_size, mean, std)\n",
        "    )\n",
        "    tst_dataset = ImageDataset(\n",
        "        test_csv, \n",
        "        test_dir, \n",
        "        transform=get_transform(img_size, mean, std)\n",
        "    )\n",
        "\n",
        "    print(f\"Training dataset size: {len(trn_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(tst_dataset)}\")\n",
        "    return trn_dataset, tst_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_device(x, device):\n",
        "    return x.to(device) if isinstance(x, torch.Tensor) else x\n",
        "\n",
        "class DeviceDataLoader:\n",
        "    def __init__(self, dataloader, device):\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.dataloader:\n",
        "            yield tuple(to_device(x, self.device) for x in batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "메트릭"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomMetrics(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.f1_macro = F1Score(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
        "        self.accuracy_per_class = classification.MulticlassAccuracy(num_classes=num_classes, average=None)\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        # preds: (batch_size, num_classes)\n",
        "        # target: (batch_size,)\n",
        "        if preds.dim() == 2:\n",
        "            preds = preds.argmax(dim=1)        \n",
        "        # F1 Macro 계산\n",
        "        f1_macro = self.f1_macro(preds, target)\n",
        "\n",
        "        # 각 클래스별 Accuracy 계산\n",
        "        accuracies = self.accuracy_per_class(preds, target)\n",
        "\n",
        "        # 가장 낮은 Accuracy 찾기\n",
        "        lowest_accuracy = torch.min(accuracies)\n",
        "\n",
        "        return {\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"lowest_class_accuracy\": lowest_accuracy,\n",
        "            \"accuracies_per_class\": accuracies\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TTA 모듈 -> 차후 분석위한 분석툴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TTA:\n",
        "    def __init__(self, model, config, threshold = 0.95, temperature = 0.5,  tta_transforms = [\n",
        "            transforms.Lambda(lambda x: x),  # 원본\n",
        "            transforms.Lambda(lambda x: transforms.functional.rotate(x, 90)),  # 90도 회전\n",
        "            transforms.Lambda(lambda x: transforms.functional.rotate(x, 180)),  # 180도 회전\n",
        "            transforms.Lambda(lambda x: transforms.functional.rotate(x, 270)),  # 270도 회전\n",
        "            transforms.Lambda(lambda x: transforms.functional.hflip(x)),  # 수평 뒤집기\n",
        "            transforms.Lambda(lambda x: transforms.functional.vflip(x)),  # 수직 뒤집기\n",
        "            transforms.Compose([  # 90도 회전 + 수평 뒤집기\n",
        "                transforms.Lambda(lambda x: transforms.functional.rotate(x, 90)),\n",
        "                transforms.Lambda(lambda x: transforms.functional.hflip(x))\n",
        "            ]),\n",
        "            transforms.Compose([  # 270도 회전 + 수평 뒤집기\n",
        "                transforms.Lambda(lambda x: transforms.functional.rotate(x, 270)),\n",
        "                transforms.Lambda(lambda x: transforms.functional.hflip(x))\n",
        "            ])\n",
        "        ]):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.threshold = threshold\n",
        "        self.temperature = temperature\n",
        "        self.tta_transforms = tta_transforms\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def tta_inference(self, image):\n",
        "        self.model.eval()\n",
        "        probs = []\n",
        "        for transform in self.tta_transforms:\n",
        "            augmented = transform(image)\n",
        "            output = self.model(augmented.unsqueeze(0).to(self.device))\n",
        "            prob = F.softmax(output, dim=1)\n",
        "            probs.append(prob.squeeze().cpu())\n",
        "        return torch.stack(probs)\n",
        "\n",
        "    def aggregate_predictions(self, probs, method):\n",
        "        if method == 'mean':\n",
        "            result = probs.mean(dim=0)\n",
        "        elif method == 'max':\n",
        "            result = probs.max(dim=0)[0]\n",
        "        elif method == 'temp_sharpen':\n",
        "            result = self._temp_sharpen(probs, self.temperature)\n",
        "        elif method == 'mode':\n",
        "            result = self._mode(probs)\n",
        "        elif method == 'modethreshold':\n",
        "            result = self._mode_threshold(probs, self.threshold)\n",
        "        elif method == 'no_tta':\n",
        "            result = probs[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported TTA aggregation method: {method}\")\n",
        "    \n",
        "        # 결과를 softmax 처리\n",
        "        return F.softmax(result, dim=0)\n",
        "\n",
        "    def _temp_sharpen(self, probs, temperature = 0.5):\n",
        "        sharpened = probs ** (1 / temperature)\n",
        "        return sharpened.mean(dim=0) / sharpened.mean(dim=0).sum()\n",
        "\n",
        "    def _mode(self, probs):\n",
        "        labels = probs.argmax(dim=1)\n",
        "        mode = Counter(labels.tolist()).most_common(1)[0][0]\n",
        "        result = torch.zeros(probs.shape[1])\n",
        "        result[mode] = 1\n",
        "        return result\n",
        "\n",
        "    def _mode_threshold(self, probs, threshold = 0.95):\n",
        "        high_conf = (probs > threshold).any(dim=1)\n",
        "        if high_conf.any():\n",
        "            high_conf_labels = probs[high_conf].argmax(dim=1)\n",
        "            mode = Counter(high_conf_labels.tolist()).most_common(1)[0][0]\n",
        "        else:\n",
        "            mode = Counter(probs.argmax(dim=1).tolist()).most_common(1)[0][0]\n",
        "        result = torch.zeros(probs.shape[1])\n",
        "        result[mode] = 1\n",
        "        return result\n",
        "        \n",
        "    def _ensemble_predictions(self, all_preds):\n",
        "        df = pd.DataFrame(all_preds)\n",
        "        mode_preds = df.mode(axis=1)\n",
        "        \n",
        "        ensemble_preds = []\n",
        "        for i in range(len(df)):\n",
        "            if len(mode_preds.iloc[i].dropna()) == 1:\n",
        "                # 동점이 없는 경우\n",
        "                ensemble_preds.append(mode_preds.iloc[i, 0])\n",
        "            else:\n",
        "                # 동점이 있는 경우\n",
        "                mode_group = set(mode_preds.iloc[i].dropna())\n",
        "                if df.loc[i, 'max'] in mode_group:\n",
        "                    ensemble_preds.append(df.loc[i, 'max'])\n",
        "                else:\n",
        "                    # max가 모드 그룹에 없을 경우 우선순위에 따라 선택\n",
        "                    for method in ['temp_sharpen', 'modethreshold', 'mode', 'mean', 'no_tta']:\n",
        "                        if df.loc[i, method] in mode_group:\n",
        "                            ensemble_preds.append(df.loc[i, method])\n",
        "                            break\n",
        "                    else:\n",
        "                        # 모든 방법이 모드 그룹에 없는 경우 그냥 max(거의 일어나지 않을 것임)\n",
        "                        ensemble_preds.append(mode_preds.iloc[i, 'max'])\n",
        "        return ensemble_preds\n",
        "    \n",
        "    def _save_dataframe_as_image(self, df, filename='df_accuracy_table.png'):\n",
        "        fig, ax = plt.subplots(figsize=(12, len(df) * 0.5 + 1))  # 행 수에 따라 세로 크기 조정\n",
        "        ax.axis('tight')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        table = ax.table(cellText=df.values,\n",
        "                         colLabels=df.columns,\n",
        "                         rowLabels=df.index,\n",
        "                         cellLoc='center',\n",
        "                         loc='center')\n",
        "        \n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(10)\n",
        "        table.scale(1.2, 1.2)\n",
        "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"Table saved as {filename}\")   \n",
        "\n",
        "    def tta_predict(self, loader):\n",
        "        methods = ['no_tta', 'mean', 'max', 'temp_sharpen', 'mode', 'modethreshold']\n",
        "        all_preds = {method: [] for method in methods}\n",
        "        all_probs = {method: [] for method in methods}\n",
        "        all_targets = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(loader, desc=\"Generating predictions\"):\n",
        "                images = images.to(self.device)\n",
        "                all_targets.append(targets)\n",
        "                for image in images:\n",
        "                    probs = self.tta_inference(image)\n",
        "                    for method in methods:\n",
        "                        agg_prob = self.aggregate_predictions(probs, method)\n",
        "                        all_preds[method].append(agg_prob.argmax().item())\n",
        "                        all_probs[method].append(agg_prob.cpu().numpy())\n",
        "\n",
        "        all_targets_tensor = torch.cat(all_targets)\n",
        "        ensemble_preds = self._ensemble_predictions(all_preds)\n",
        "        all_preds['ensemble'] = ensemble_preds\n",
        "        # Note: We don't add 'ensemble' to all_probs\n",
        "\n",
        "        return all_preds, all_probs, all_targets_tensor\n",
        "\n",
        "    def tta_evaluate(self, loader):\n",
        "        all_preds, all_probs, all_targets = self.tta_predict(loader)\n",
        "        \n",
        "        results = {}\n",
        "        metrics = CustomMetrics(self.config.num_classes)\n",
        "        \n",
        "        for method in all_preds.keys():\n",
        "            preds = torch.tensor(all_preds[method])\n",
        "            targets = all_targets.clone().detach()\n",
        "            metric_results = metrics(preds, targets)\n",
        "            results[method] = {\n",
        "                'preds': preds,\n",
        "                'probs': all_probs.get(method) if method != 'ensemble' else None,\n",
        "                'f1_macro': metric_results['f1_macro'].item(),\n",
        "                'lowest_class_accuracy': metric_results['lowest_class_accuracy'].item(),\n",
        "                'accuracies_per_class': metric_results['accuracies_per_class'].tolist()\n",
        "            }\n",
        "        \n",
        "        return results, all_targets\n",
        "    \n",
        "    def save_predictions(self, predictions, probs, output_dir='submissions'):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        sample_submission_path = os.path.join(self.config.base_path, \"sample_submission.csv\")\n",
        "        sample_submission = pd.read_csv(sample_submission_path)\n",
        "        \n",
        "        for method, preds in predictions.items():\n",
        "            # 예측 결과 저장\n",
        "            submission = sample_submission.copy()\n",
        "            submission['target'] = preds\n",
        "            try: \n",
        "                pred_filename = f'{self.config.group_name}_submission_tta_{method}.csv'\n",
        "            except:\n",
        "                pred_filename = f'data_orig_submission_tta_{method}.csv'\n",
        "            pred_path =  f'{output_dir}/{pred_filename}'\n",
        "            submission.to_csv(pred_path, index=False)\n",
        "            print(f\"Saved predictions for method: {method}\")\n",
        "            \n",
        "            # 확률 저장\n",
        "            if method != 'ensemble' and probs is not None:\n",
        "                prob_df = pd.DataFrame(probs[method], columns=[f'prob_{i}' for i in range(len(probs[method][0]))])\n",
        "                try:\n",
        "                    prob_filename = f'{self.config.group_name}_labelprob_tta_{method}.csv'\n",
        "                except:\n",
        "                    prob_filename = f'data_orig_labelprob_tta_{method}.csv'\n",
        "                prob_dir = os.path.join(output_dir, 'probs')\n",
        "                os.makedirs(prob_dir, exist_ok=True)\n",
        "                prob_path = f'{prob_dir}/{prob_filename}'\n",
        "                prob_df.to_csv(prob_path, index=False)\n",
        "                print(f\"Saved label probabilities for method: {method}\")\n",
        "\n",
        "    def automatic_tta_submission(self, test_loader, output_dir='submissions'):\n",
        "        predictions, probs, dummy_target = self.tta_predict(test_loader)\n",
        "        self.save_predictions(predictions, probs)   \n",
        "\n",
        "    def analyze_predictions(self, preds, targets, method ='no_tta', chart_dir= 'charts/data_orig'):\n",
        "        os.makedirs(chart_dir, exist_ok=True)\n",
        "        preds = preds.clone().detach()\n",
        "        targets = targets.clone().detach()\n",
        "        accuracy_per_class = []\n",
        "        for class_idx in range(self.config.num_classes):\n",
        "            class_mask = (targets == class_idx)\n",
        "            if class_mask.sum() > 0:\n",
        "                class_accuracy = (preds[class_mask] == targets[class_mask]).float().mean().item()\n",
        "            else:\n",
        "                class_accuracy = 0.0\n",
        "            accuracy_per_class.append(class_accuracy)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(range(len(accuracy_per_class)), accuracy_per_class)\n",
        "        plt.title(f'Accuracy per Class - {method}')\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Accuracy')\n",
        "        try: \n",
        "            plt.xticks(range(len(self.config.class_names)), self.config.class_names, rotation=90)\n",
        "        except:\n",
        "            plt.xticks(range(self.config.num_classes))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(chart_dir, f'accuracy_per_class_{method}.png'))\n",
        "        plt.close()\n",
        "        \n",
        "        conf_matrix = confusion_matrix(targets, preds)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Confusion Matrix - {method}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        try: \n",
        "            plt.xticks(range(len(self.config.class_names)), self.config.class_names, rotation=90)\n",
        "            plt.yticks(range(len(self.config.class_names)), self.config.class_names, rotation=0)\n",
        "        except:\n",
        "            pass\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(chart_dir, f'confusion_matrix_{method}.png'))\n",
        "        plt.close()\n",
        "        \n",
        "        return accuracy_per_class\n",
        "\n",
        "    def analyze_tta(self, val_loader):\n",
        "        results, targets = self.tta_evaluate(val_loader)\n",
        "        targets_tensor = torch.as_tensor(targets)\n",
        "\n",
        "        accuracy_per_class_all = {}\n",
        "        \n",
        "        # 차트 저장 경로 설정\n",
        "        try:\n",
        "            disagreement_dir = f'disagreement_samples/{self.config.group_name}'\n",
        "            chart_dir = f'charts/{self.config.group_name}'\n",
        "        except AttributeError:\n",
        "            chart_dir = os.path.join('charts', 'data_orig')\n",
        "            disagreement_dir = os.path.join('disagreement_samples','data_orig')\n",
        "        os.makedirs(disagreement_dir, exist_ok=True)\n",
        "        os.makedirs(chart_dir, exist_ok=True)\n",
        "        \n",
        "        for method in results.keys():\n",
        "            print(f'Summary Results for the aggregate method: {method}')\n",
        "            accuracy_per_class = self.analyze_predictions(results[method]['preds'], targets, method, chart_dir)\n",
        "            accuracy_per_class_all[method] = accuracy_per_class\n",
        "        \n",
        "        # Find images with disagreement\n",
        "        disagreement_mask = torch.zeros(len(targets_tensor), dtype=torch.bool, device=targets_tensor.device)\n",
        "        for method in results.keys():\n",
        "            if method != 'ensemble':\n",
        "                preds_tensor = torch.as_tensor(results[method]['preds'], device=targets_tensor.device)\n",
        "                disagreement_mask |= (preds_tensor != targets_tensor)\n",
        "\n",
        "        disagreement_indices = disagreement_mask.nonzero().squeeze()\n",
        "        \n",
        "        # Count disagreements per class\n",
        "        disagreement_counts = torch.bincount(targets.clone().detach()[disagreement_indices], minlength=self.config.num_classes)\n",
        "        \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(range(self.config.num_classes), disagreement_counts)\n",
        "        plt.title('Disagreements per Class')\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Number of Disagreements')\n",
        "        try: \n",
        "            plt.xticks(range(len(self.config.class_names)), self.config.class_names, rotation=90)\n",
        "        except:\n",
        "            plt.xticks(range(self.config.num_classes))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(chart_dir, 'disagreements_per_class.png'))\n",
        "        plt.close()\n",
        "        \n",
        "        # Save disagreement images\n",
        "        dataset = val_loader.dataset\n",
        "        for idx in disagreement_indices[:min(len(disagreement_indices), 100)]:  # Save up to 100 samples\n",
        "            try:\n",
        "                img, _ = dataset[idx]\n",
        "                img_np = img.permute(1, 2, 0).numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # Normalize to [0, 1]\n",
        "                img_path = os.path.join(disagreement_dir, f'disagreement_sample_{idx}.png')\n",
        "                plt.imsave(img_path, img_np)\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving disagreement sample {idx}: {str(e)}\")\n",
        "        print(f\"Saved disagreement samples to {disagreement_dir}\")\n",
        "        \n",
        "        # Create a new dataset with only disagreement samples\n",
        "        disagreement_dataset = torch.utils.data.Subset(dataset, disagreement_indices)\n",
        "        \n",
        "        # Create DataFrame for accuracy per class\n",
        "        try: \n",
        "            class_names = self.config.class_names\n",
        "        except:\n",
        "            class_names = [f\"Class {i}\" for i in range(self.config.num_classes)]\n",
        "        df_accuracy = pd.DataFrame(accuracy_per_class_all).T\n",
        "        df_accuracy.columns = class_names\n",
        "        \n",
        "        # Add average accuracy row\n",
        "        df_accuracy.loc['Average'] = df_accuracy.mean()\n",
        "        \n",
        "        # Round all values to 4 decimal places\n",
        "        df_accuracy = df_accuracy.round(4)\n",
        "        \n",
        "        print(\"\\nAccuracy per class for all methods:\")\n",
        "        display(df_accuracy)\n",
        "        # Save the dataframe as an image\n",
        "        self._save_dataframe_as_image(df_accuracy, os.path.join(chart_dir, 'accuracy_per_class_all_methods.png'))\n",
        "\n",
        "        return disagreement_dataset, df_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mixup & Cutmix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)  # np.int() 대신 int() 사용\n",
        "    cut_h = int(H * cut_rat)  # np.int() 대신 int() 사용\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    y_a, y_b = y, y[index]\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
        "\n",
        "    return x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "커스텀 모델 -> 수정, 보완 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.config = config\n",
        "        self.backbone = timm.create_model(\n",
        "            config.model_name,\n",
        "            pretrained=config.pretrained,\n",
        "            num_classes=0,\n",
        "            in_chans=3\n",
        "        )\n",
        "        \n",
        "        if hasattr(self.backbone, 'num_features'):\n",
        "            num_features = self.backbone.num_features\n",
        "        elif hasattr(self.backbone, 'fc'):\n",
        "            num_features = self.backbone.fc.in_features\n",
        "        else:\n",
        "            raise ValueError(\"Unable to determine number of features in backbone\")\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, config.num_classes)\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)        \n",
        "        self.metrics = CustomMetrics(num_classes=config.num_classes).to(config.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        out = self.classifier(features)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, loss_func):\n",
        "        x, y = batch\n",
        "        rand_num = np.random.rand()\n",
        "        if rand_num < self.config.mixup_prob:\n",
        "            mixed_x, y_a, y_b, lam = mixup_data(x, y, self.config.mixup_alpha)\n",
        "            logits = self(mixed_x)\n",
        "            loss = mixup_criterion(loss_func, logits, y_a, y_b, lam)\n",
        "        elif rand_num < self.config.mixup_prob + self.config.cutmix_prob:\n",
        "            mixed_x, y_a, y_b, lam = cutmix_data(x, y, self.config.cutmix_alpha)\n",
        "            logits = self(mixed_x)\n",
        "            loss = cutmix_criterion(loss_func, logits, y_a, y_b, lam)\n",
        "        else:\n",
        "            logits = self(x)\n",
        "            loss = loss_func(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        return loss, preds\n",
        "\n",
        "    def validation_step(self, batch, loss_func):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        loss = loss_func(logits, y)\n",
        "        return loss, preds\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        x, labels  = batch\n",
        "        logits = self(x)\n",
        "        prob = self.softmax(logits)\n",
        "        return prob, labels "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "손실함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmoothFocalLoss(nn.Module):\n",
        "    def __init__(self, num_classes, alpha=None, gamma=2, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.gamma = gamma\n",
        "        self.smoothing = smoothing\n",
        "        \n",
        "        if alpha is None:\n",
        "            self.alpha = torch.ones(num_classes)\n",
        "        else:\n",
        "            self.alpha = torch.tensor(alpha)\n",
        "        \n",
        "        self.alpha = self.alpha / self.alpha.sum()\n",
        "        \n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "        \n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "        \n",
        "        # 라벨 스무딩 적용\n",
        "        targets_smooth = (1 - self.smoothing) * targets + self.smoothing / self.num_classes\n",
        "        \n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -targets_smooth * log_probs\n",
        "        \n",
        "        pt = torch.exp(-loss)\n",
        "        focal_loss = (1 - pt)**self.gamma * loss\n",
        "        \n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha.to(inputs.device)\n",
        "            focal_loss = alpha.unsqueeze(0) * focal_loss\n",
        "        \n",
        "        return focal_loss.sum(dim=1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, num_classes, alpha=None, gamma=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        if alpha is None:\n",
        "            self.alpha = torch.ones(num_classes)\n",
        "        else:\n",
        "            self.alpha = torch.tensor(alpha)\n",
        "        \n",
        "        self.alpha = self.alpha / self.alpha.sum()\n",
        "        \n",
        "    def __call__(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "        \n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "        \n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt)**self.gamma * ce_loss\n",
        "        \n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha.to(inputs.device)\n",
        "            focal_loss = alpha.unsqueeze(0) * focal_loss\n",
        "        \n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CELoss(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "    def __call__(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "        \n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "        \n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        \n",
        "        return ce_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmoothCELoss(nn.Module):\n",
        "    def __init__(self, num_classes, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.smoothing = smoothing\n",
        "        \n",
        "    def __call__(self, inputs, targets):\n",
        "        inputs = inputs.float()\n",
        "        \n",
        "        # 타겟이 클래스 인덱스인 경우 원-핫 인코딩으로 변환\n",
        "        if targets.dim() == 1:\n",
        "            targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "        \n",
        "        # 라벨 스무딩 적용\n",
        "        targets_smooth = (1 - self.smoothing) * targets + self.smoothing / self.num_classes\n",
        "        \n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -targets_smooth * log_probs\n",
        "        \n",
        "        return loss.sum(dim=1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_loss_function(config):\n",
        "    if config.loss == 'ce':\n",
        "        return CELoss(num_classes=config.num_classes)\n",
        "    elif config.loss == 'focal':\n",
        "        return FocalLoss(num_classes=config.num_classes, alpha=config.focal_alpha, gamma=config.focal_gamma)\n",
        "    elif config.loss == 'smoothce':\n",
        "        return SmoothCELoss(num_classes=config.num_classes, smoothing=config.smoothing)\n",
        "    elif config.loss == 'smoothfocal':\n",
        "        return SmoothFocalLoss(num_classes=config.num_classes, alpha=config.focal_alpha, gamma=config.focal_gamma, smoothing=config.smoothing)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported loss function: {config.loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "옵티마이저"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Ranger(optim.Optimizer): # from torch.optim.optimizer import Optimizer\n",
        "    def __init__(self, params, lr=1e-3,                       # lr\n",
        "                 alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n",
        "                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
        "                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
        "                 use_gc=True, gc_conv_only=False\n",
        "                 ):\n",
        "\n",
        "        # parameter checks\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        if not lr > 0:\n",
        "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
        "        if not eps > 0:\n",
        "            raise ValueError(f'Invalid eps: {eps}')\n",
        "\n",
        "\n",
        "        # prep defaults and init torch.optim base\n",
        "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
        "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        # adjustable threshold\n",
        "        self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "        # look ahead params\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "\n",
        "        # radam buffer for state\n",
        "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
        "\n",
        "        # gc on or off\n",
        "        self.use_gc = use_gc\n",
        "\n",
        "        # level of gradient centralization\n",
        "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
        "\n",
        "        print(\n",
        "            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n",
        "        if (self.use_gc and self.gc_gradient_threshold == 1):\n",
        "            print(f\"GC applied to both conv and fc layers\")\n",
        "        elif (self.use_gc and self.gc_gradient_threshold == 3):\n",
        "            print(f\"GC applied to conv layers only\")\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        print(\"set state called\")\n",
        "        super(Ranger, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "\n",
        "        # Evaluate averages and grad, update param tensors\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        'Ranger optimizer does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]  # get state dict for this param\n",
        "\n",
        "                if len(state) == 0:  \n",
        "\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                    # look ahead weight storage now in state dict\n",
        "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
        "                    state['slow_buffer'].copy_(p.data)\n",
        "\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
        "                        p_data_fp32)\n",
        "\n",
        "                # begin computations\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # GC operation for Conv layers and FC layers\n",
        "                if grad.dim() > self.gc_gradient_threshold:\n",
        "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # compute variance mov avg\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                # compute mean moving avg\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * \\\n",
        "                        state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > self.N_sma_threshhold:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
        "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay']\n",
        "                                     * group['lr'], p_data_fp32)\n",
        "\n",
        "                # apply lr\n",
        "                if N_sma > self.N_sma_threshhold:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size *\n",
        "                                         group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "                # integrated look ahead...\n",
        "                # we do it at the param level instead of group level\n",
        "                if state['step'] % group['k'] == 0:\n",
        "                    # get access to slow param tensor\n",
        "                    slow_p = state['slow_buffer']\n",
        "                    # (fast weights - slow weights) * alpha\n",
        "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
        "                    # copy interpolated weights to RAdam param tensor\n",
        "                    p.data.copy_(slow_p)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimizer(model_params, config):\n",
        "    if config.optimizer == 'SGD':\n",
        "        return optim.SGD(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            momentum=config.momentum,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    elif config.optimizer == 'Adam':\n",
        "        return optim.Adam(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            betas=(config.adam_beta1, config.adam_beta2),\n",
        "            eps=config.adam_epsilon,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    elif config.optimizer == 'AdamW':\n",
        "        return optim.AdamW(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            betas=(config.adam_beta1, config.adam_beta2),\n",
        "            eps=config.adam_epsilon,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    elif config.optimizer == 'RMSprop':\n",
        "        return optim.RMSprop(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            alpha=config.rmsprop_alpha,\n",
        "            eps=config.rmsprop_epsilon,\n",
        "            weight_decay=config.weight_decay,\n",
        "            momentum=config.momentum\n",
        "        )\n",
        "    elif config.optimizer == 'Adadelta':\n",
        "        return optim.Adadelta(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            rho=config.adadelta_rho,\n",
        "            eps=config.adadelta_epsilon,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    elif config.optimizer == 'Ranger':\n",
        "        return Ranger(\n",
        "            model_params,\n",
        "            lr=config.learning_rate,\n",
        "            alpha=config.ranger_alpha,\n",
        "            k=config.ranger_k,\n",
        "            N_sma_threshhold=config.ranger_N_sma_threshhold,\n",
        "            betas=(config.ranger_beta1, config.ranger_beta2),\n",
        "            eps=config.ranger_eps,\n",
        "            weight_decay=config.weight_decay,\n",
        "            use_gc=config.ranger_use_gc,\n",
        "            gc_conv_only=config.ranger_gc_conv_only\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {config.optimizer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "스케쥴러"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_scheduler(optimizer, config):\n",
        "    if config.scheduler == 'LambdaLR':\n",
        "        return optim.lr_scheduler.LambdaLR(\n",
        "            optimizer=optimizer,\n",
        "            lr_lambda=lambda epoch: config.lambda_factor ** epoch,\n",
        "            last_epoch=config.last_epoch,\n",
        "            verbose=config.verbose\n",
        "        )\n",
        "    elif config.scheduler == 'MultiplicativeLR':\n",
        "        return optim.lr_scheduler.MultiplicativeLR(\n",
        "            optimizer=optimizer,\n",
        "            lr_lambda=lambda epoch: config.lambda_factor ** epoch\n",
        "        )\n",
        "    elif config.scheduler == 'StepLR':\n",
        "        return optim.lr_scheduler.StepLR(\n",
        "            optimizer, \n",
        "            step_size=config.step_size, \n",
        "            gamma=config.gamma\n",
        "        )\n",
        "    elif config.scheduler == 'CosineAnnealingLR':\n",
        "        return optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, \n",
        "            T_max=config.T_max, \n",
        "            eta_min=config.eta_min\n",
        "        )\n",
        "    elif config.scheduler == 'ReduceLROnPlateau':\n",
        "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, \n",
        "            mode=config.mode\n",
        "        )\n",
        "    elif config.scheduler == 'CyclicLR':\n",
        "        return optim.lr_scheduler.CyclicLR(\n",
        "            optimizer, \n",
        "            base_lr=config.base_lr,\n",
        "            max_lr=config.max_lr,\n",
        "            step_size_up=config.step_size_up,\n",
        "            mode=config.mode,\n",
        "            gamma=config.gamma\n",
        "        )\n",
        "    elif config.scheduler == 'OneCycleLR':\n",
        "        return optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer, \n",
        "            max_lr=config.max_lr,\n",
        "            steps_per_epoch=config.steps_per_epoch, \n",
        "            epochs=config.epochs,\n",
        "            anneal_strategy=config.anneal_strategy\n",
        "        )\n",
        "    elif config.scheduler == 'CosineAnnealingWarmRestarts':\n",
        "        return optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, \n",
        "            T_0=config.T_0,\n",
        "            T_mult=config.T_mult, \n",
        "            eta_min=config.eta_min\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {config.scheduler}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "훈련 - 트레이너"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2330: UserWarning: Run (m5njoq91) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "Epochs:   0%|          | 1/300 [11:55<59:26:57, 715.78s/it, Train Loss=0.1032, Val Loss=0.0953, Train F1=0.0853, Val F1=0.1413, Train Lowest_class_accuracy=0.0000, Val Lowest_class_accuracy=0.0000]\n"
          ]
        }
      ],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.group_name = self.config.group_name\n",
        "        self.save_config()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.best_val_f1 = 0\n",
        "        self.start_epoch = 0\n",
        "        self.total_epochs = self.config.epochs\n",
        "        self.loss_func = self.get_loss_function()\n",
        "        self.optimizer = self.get_optimizer()\n",
        "        self.scheduler = self.get_scheduler()\n",
        "        self.scaler = GradScaler(enabled=config.use_mixed_precision)\n",
        "        self.early_stopping_counter = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.metrics = CustomMetrics(num_classes=config.num_classes).to(self.device)\n",
        "        \n",
        "        self.use_mixed_precision = config.use_mixed_precision\n",
        "        if self.use_mixed_precision:\n",
        "            self.scaler = GradScaler()\n",
        "\n",
        "    #################### 기본 세팅 ####################\n",
        "    def initialize_model(self):\n",
        "        return CustomModel(self.config).to(self.config.device)\n",
        "\n",
        "    def get_loss_function(self):\n",
        "        return get_loss_function(self.config)\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return get_optimizer(self.model.parameters(), self.config)\n",
        "\n",
        "    def get_scheduler(self):\n",
        "        return get_scheduler(self.optimizer, self.config)\n",
        "    \n",
        "    ##########################################################\n",
        "    @staticmethod\n",
        "    def get_short_model_name(model_name):\n",
        "        return model_name.split('.')[0]\n",
        "\n",
        "    def generate_new_group_name(self, old_group_name = False):\n",
        "        if old_group_name:\n",
        "            parts = old_group_name.rsplit('_', 1)\n",
        "            new_suffix = wandb.util.generate_id()[:8]\n",
        "            return f\"{parts[0]}_{new_suffix}\"\n",
        "        else:\n",
        "            model_short_name = self.get_short_model_name(self.config.model_name)\n",
        "            return f\"{model_short_name}_{self.config.optimizer}_{wandb.util.generate_id()[:8]}\"\n",
        "    \n",
        "    ################세이브 로드 ####################\n",
        "    def save_checkpoint(self, epoch, val_f1):\n",
        "        checkpoint_path = f'models/{self.group_name}/checkpoint_epoch_{epoch+1}.pt'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'config': dict(self.config),\n",
        "            'val_f1_macro': val_f1,\n",
        "        }, checkpoint_path)\n",
        "        if val_f1 > self.best_val_f1:\n",
        "            self.best_val_f1 = val_f1\n",
        "            best_model_path = f'models/{self.group_name}/best_model.pt'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "                'config': dict(self.config),\n",
        "                'val_f1_macro': val_f1,\n",
        "            }, best_model_path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Model file not found: {path}\")\n",
        "            return False\n",
        "        try:\n",
        "            checkpoint = torch.load(path, map_location=self.device)\n",
        "            self.config.__dict__.update(checkpoint['config'])\n",
        "            self.model = CustomModel(self.config).to(self.device)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            self.best_val_f1 = checkpoint['val_f1_macro']\n",
        "            self.group_name = self.config.group_name\n",
        "            print(f\"Model loaded from {path}\")\n",
        "            print(f\"Continuing training from epoch {self.start_epoch}\")\n",
        "            print(f\"Best validation F1 score: {self.best_val_f1:.4f}\")\n",
        "            self.model.train()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {path}: {str(e)}\")\n",
        "            return False\n",
        "        \n",
        "    def load_best_model(self, group_name=None):\n",
        "        if group_name is None:\n",
        "            group_name = self.group_name\n",
        "        best_model_path = os.path.join('models', group_name, 'best_model.pt')\n",
        "        if not os.path.exists(best_model_path):\n",
        "            print(f\"No best model found for group {group_name}\")\n",
        "            return False\n",
        "        return self.load_model(best_model_path)\n",
        "    \n",
        "    ######################## Config 저장 #######################\n",
        "    def save_config(self):\n",
        "        config_path = f'models/{self.group_name}/config.json'\n",
        "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(dict(self.config), f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_config(group_name):\n",
        "        config_path = f'models/{group_name}/config.json'\n",
        "        with open(config_path, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "        return Config(**config_dict)\n",
        "    \n",
        "    #################### 훈련 스크립트 ####################\n",
        "    def train_epoch_mixed_precision(self, train_loader, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        metrics = self.initialize_metrics()\n",
        "        pbar = tqdm_notebook(train_loader, desc=f\"Epoch {epoch+1}/{self.total_epochs}\", leave=False)\n",
        "        for batch in pbar:\n",
        "            x, y = batch\n",
        "            with autocast():\n",
        "                loss, preds = self.model.training_step(batch, self.loss_func)\n",
        "            self.scaler.scale(loss).backward()\n",
        "            if self.config.gradient_clip_val > 0:\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "            self.optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "            self.update_metrics(metrics, {\"preds\": preds, \"targets\": y})\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "        return total_loss / len(train_loader), self.calculate_metrics(metrics)\n",
        "\n",
        "    def train_epoch_full_precision(self, train_loader, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        metrics = self.initialize_metrics()\n",
        "        pbar = tqdm_notebook(train_loader, desc=f\"Epoch {epoch+1}/{self.total_epochs}\", leave=False)\n",
        "        for batch in pbar:\n",
        "            x, y = batch\n",
        "            loss, preds = self.model.training_step(batch, self.loss_func)\n",
        "            loss.backward()\n",
        "            if self.config.gradient_clip_val > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "            self.update_metrics(metrics, {\"preds\": preds, \"targets\": y})\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "        return total_loss / len(train_loader), self.calculate_metrics(metrics)\n",
        "    \n",
        "    def validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        metrics = self.initialize_metrics()\n",
        "        val_pbar = tqdm_notebook(val_loader, desc=\"Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_pbar:\n",
        "                x, y = batch\n",
        "                if self.config.use_mixed_precision:\n",
        "                    with autocast():\n",
        "                        loss, preds = self.model.validation_step(batch, self.loss_func)\n",
        "                else:\n",
        "                    loss, preds = self.model.validation_step(batch, self.loss_func)\n",
        "                total_loss += loss.item()\n",
        "                self.update_metrics(metrics, {\"preds\": preds, \"targets\": y})\n",
        "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "        return total_loss / len(val_loader), self.calculate_metrics(metrics)\n",
        "        \n",
        "    def train(self, train_loader, validation_loader, is_validate = False):\n",
        "        wandb.init(project=self.config.project_name, config=vars(self.config), group=self.group_name, job_type= \"new_run\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        epoch_pbar = tqdm_notebook(range(self.start_epoch, self.total_epochs), desc=\"Epochs\", leave=True)\n",
        "        for epoch in epoch_pbar:\n",
        "            epoch_start_time = time.time()\n",
        "            # Training\n",
        "            if self.use_mixed_precision:\n",
        "                train_loss, train_metrics = self.train_epoch_mixed_precision(train_loader, epoch)\n",
        "            else:\n",
        "                train_loss, train_metrics = self.train_epoch_full_precision(train_loader, epoch)\n",
        "            # Validation\n",
        "            if is_validate:\n",
        "                valid_loss, valid_metrics = self.validate(validation_loader)\n",
        "            else:\n",
        "                valid_loss, valid_metrics = 0, {'f1_macro': 0, 'lowest_class_accuracy': 0}\n",
        "\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            self.log_metrics(epoch, train_loss, train_metrics, valid_loss, valid_metrics, is_validate)\n",
        "            self.update_scheduler(train_loss)\n",
        "            self.save_checkpoint(epoch, train_metrics['f1_macro'])\n",
        "\n",
        "            if is_validate:\n",
        "                print(f\"Epoch {epoch+1}/{self.total_epochs} - \"\n",
        "                    f\"Loss: {train_loss:.4f}, F1 Macro: {train_metrics['f1_macro']:.4f}, \"\n",
        "                    f\"Lowest_class_accuracy: {train_metrics['lowest_class_accuracy']:.4f}, \"\n",
        "                    f\"Val Loss: {valid_loss:.4f}, Val F1 Macro: {valid_metrics['f1_macro']:.4f}, \"\n",
        "                    f\"Val Lowest_class_accuracy: {valid_metrics['lowest_class_accuracy']:.4f}, Time: {epoch_time:.2f}s\")  \n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{self.total_epochs} - \"\n",
        "                    f\"Loss: {train_loss:.4f}, F1 Macro: {train_metrics['f1_macro']:.4f}, \"\n",
        "                    f\"Lowest_class_accuracy: {train_metrics['lowest_class_accuracy']:.4f}, Time: {epoch_time:.2f}s\")  \n",
        "                   \n",
        "            # Update epoch progress bar\n",
        "            if is_validate:\n",
        "                epoch_pbar.set_postfix({\n",
        "                    'Train Loss': f'{train_loss:.4f}',\n",
        "                    'Val Loss': f'{valid_loss:.4f}',\n",
        "                    'Train F1': f'{train_metrics[\"f1_macro\"]:.4f}',\n",
        "                    'Val F1': f'{valid_metrics[\"f1_macro\"]:.4f}',\n",
        "                    'Train Lowest_class_accuracy': f\"{train_metrics['lowest_class_accuracy']:.4f}\",\n",
        "                    'Val Lowest_class_accuracy': f\"{valid_metrics['lowest_class_accuracy']:.4f}\",\n",
        "                })\n",
        "            else:\n",
        "                epoch_pbar.set_postfix({\n",
        "                    'Train Loss': f'{train_loss:.4f}',\n",
        "                    'Train F1': f'{train_metrics[\"f1_macro\"]:.4f}',\n",
        "                    'Train Lowest_class_accuracy': f\"{train_metrics['lowest_class_accuracy']:.4f}\"\n",
        "                })            \n",
        "            if self.early_stopping(train_loss):\n",
        "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Training completed. Total time: {total_time:.2f}s\")\n",
        "        return train_loss, train_metrics, valid_loss, valid_metrics\n",
        "    \n",
        "    def setup_training(self, checkpoint_path=None, continue_training=False, additional_epochs=0):\n",
        "        if checkpoint_path:\n",
        "            self.load_model(checkpoint_path)\n",
        "        else:\n",
        "            self.load_model(checkpoint_path)\n",
        "            self.start_epoch = 0\n",
        "            self.best_val_f1 = 0\n",
        "            self.group_name = self.generate_new_group_name()\n",
        "\n",
        "        if continue_training:\n",
        "            self.group_name = self.generate_new_group_name(self.group_name)\n",
        "            print(f\"Continuing training with new group_name: {self.group_name}\")\n",
        "            self.total_epochs = self.start_epoch + additional_epochs\n",
        "        else:\n",
        "            self.total_epochs = self.config.epochs\n",
        "\n",
        "        self.config.group_name = self.group_name\n",
        "        self.model.train()\n",
        "        self.scaler = GradScaler(enabled=self.config.use_mixed_precision)\n",
        "        \n",
        "        if continue_training:\n",
        "            self.optimizer = self.get_optimizer()\n",
        "            self.scheduler = self.get_scheduler()\n",
        "\n",
        "    #################### 기록관련 ####################\n",
        "    def initialize_metrics(self):\n",
        "        return {\"preds\": [], \"targets\": []}\n",
        "\n",
        "    def update_metrics(self, metrics, batch_metrics):\n",
        "        metrics[\"preds\"].extend(batch_metrics[\"preds\"].cpu().numpy())\n",
        "        metrics[\"targets\"].extend(batch_metrics[\"targets\"].cpu().numpy())\n",
        "\n",
        "    def calculate_metrics(self, metrics):\n",
        "        preds = torch.from_numpy(np.array(metrics[\"preds\"])).to(self.device)\n",
        "        targets = torch.from_numpy(np.array(metrics[\"targets\"])).to(self.device)\n",
        "        return self.metrics(preds, targets)\n",
        "\n",
        "    def log_metrics(self, epoch, train_loss, train_metrics, valid_loss, valid_metrics, is_validate = False):\n",
        "        if is_validate:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_f1_macro\": train_metrics['f1_macro'],\n",
        "                \"train_lowest_class_accuracy\": train_metrics['lowest_class_accuracy'],\n",
        "                \"valid_loss\": valid_loss,\n",
        "                \"valid_f1_macro\": valid_metrics['f1_macro'],\n",
        "                \"valid_lowest_class_accuracy\": valid_metrics['lowest_class_accuracy'],\n",
        "                \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "            for i, acc in enumerate(train_metrics['accuracies_per_class']):\n",
        "                wandb.log({f\"train_class_{i}_accuracy\": acc.item()})\n",
        "            for i, acc in enumerate(valid_metrics['accuracies_per_class']):\n",
        "                wandb.log({f\"valid_class_{i}_accuracy\": acc.item()})\n",
        "        else:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_f1_macro\": train_metrics['f1_macro'],\n",
        "                \"train_lowest_class_accuracy\": train_metrics['lowest_class_accuracy'],\n",
        "                \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "            for i, acc in enumerate(train_metrics['accuracies_per_class']):\n",
        "                wandb.log({f\"train_class_{i}_accuracy\": acc.item()})       \n",
        "\n",
        "    #################### 훈련 관련 ####################\n",
        "    def update_scheduler(self, val_loss):\n",
        "        if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            self.scheduler.step(val_loss)\n",
        "        else:\n",
        "            self.scheduler.step()\n",
        "    \n",
        "    def early_stopping(self, val_loss):\n",
        "        if val_loss < self.best_val_loss - self.config.early_stopping_delta:\n",
        "            self.best_val_loss = val_loss\n",
        "            self.early_stopping_counter = 0\n",
        "        else:\n",
        "            self.early_stopping_counter += 1\n",
        "            if self.early_stopping_counter >= self.config.early_stopping_patience:\n",
        "                return True\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <세팅하기!> (이전까지 전부 실행 후 순차적 실행)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 기본세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 모델 고르시오 tiny, mobilenetv4 GMACs, GFLOPS따져서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#timm.list_models('cafo*',pretrained=True)\n",
        "timm.list_models('tiny*',pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. 기본설정 - Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 경로 및 기본설정\n",
        "base_path = 'data_orig/'\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# FineTuning 대상 변수 미리 꺼내놓기\n",
        "num_classes = 17\n",
        "learning_rate = 1e-3\n",
        "optimizer = 'Ranger' \n",
        "img_size = 1024\n",
        "batch_size = 8\n",
        "gradient_clip_val = 1 #0이면 클리핑 안함\n",
        "use_mixed_precision = True\n",
        "\n",
        "# 가중치 설정\n",
        "focal_alpha = [1]*17\n",
        "focal_alpha[3] = 3\n",
        "focal_alpha[4] = 3\n",
        "focal_alpha[7] = 4\n",
        "focal_alpha[14] = 3\n",
        "\n",
        "model_name = \"tiny_vit_5m_224.dist_in22k_ft_in1k\"\n",
        "#model_name = \"caformer_s36.sail_in22k_ft_in1k\"\n",
        "#model_name = \"caformer_m36.sail_in22k_ft_in1k\"#\n",
        "#model_name = \"tiny_vit_11m_224.dist_in22k_ft_in1k\"\n",
        "\n",
        "PROJECT_NAME = \"k-fold-cross-validation-pytorch\"\n",
        "def get_short_model_name(model_name):\n",
        "    return model_name.split('.')[0]\n",
        "model_short_name = get_short_model_name(model_name)\n",
        "GROUP_NAME = f\"{model_short_name}_{optimizer}_{wandb.util.generate_id()[:8]}\"\n",
        "JOB_TYPE = 'train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 새버전 Config k-fold, manual warmup 삭제\n",
        "config_manual = {\n",
        "    # 기본 설정\n",
        "    \"seed\": 42,\n",
        "    \"device\": device,\n",
        "    \"num_workers\": 0,\n",
        "    \"group_name\": GROUP_NAME,\n",
        "    \"base_path\": base_path,\n",
        "    \"project_name\": PROJECT_NAME,\n",
        "\n",
        "    # 데이터 관련 설정\n",
        "    \"batch_size\": batch_size,\n",
        "    \"img_size\": img_size,\n",
        "    \"num_classes\": num_classes,\n",
        "\n",
        "    # 모델 관련 설정\n",
        "    \"model_name\": model_name,  # 예시\n",
        "    \"pretrained\": True,\n",
        "\n",
        "    # 훈련 관련 설정\n",
        "    \"epochs\": 300,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"weight_decay\": 1e-5,\n",
        "\n",
        "    # 손실 함수 관련 설정\n",
        "    \"loss\": \"smoothfocal\",  # 'ce', 'focal', 'smoothce', 'smoothfocal'\n",
        "    \"focal_gamma\": 2,\n",
        "    \"smoothing\": 0.1,\n",
        "    \"focal_alpha\": focal_alpha,\n",
        "\n",
        "    # 데이터 증강 관련 설정\n",
        "    \"mixup_alpha\": 1.0,\n",
        "    \"cutmix_alpha\": 1.0,\n",
        "    \"mixup_prob\": 0.2,\n",
        "    \"cutmix_prob\": 0,\n",
        "\n",
        "    # 옵티마이저 설정\n",
        "    \"optimizer\": optimizer,  # 'SGD', 'Adam', 'AdamW', 'RMSprop', 'Adadelta', 'Ranger'\n",
        "    \"momentum\": 0.9,\n",
        "\n",
        "    # Adam과 AdamW 특정 설정\n",
        "    \"adam_beta1\": 0.9,\n",
        "    \"adam_beta2\": 0.999,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "\n",
        "    # RMSprop 특정 설정\n",
        "    \"rmsprop_alpha\": 0.99,\n",
        "    \"rmsprop_epsilon\": 1e-8,\n",
        "\n",
        "    # Adadelta 특정 설정\n",
        "    \"adadelta_rho\": 0.9,\n",
        "    \"adadelta_epsilon\": 1e-6,\n",
        "\n",
        "    # Ranger 특정 설정\n",
        "    \"ranger_alpha\": 0.5,\n",
        "    \"ranger_k\": 6,\n",
        "    \"ranger_N_sma_threshhold\": 5,\n",
        "    \"ranger_beta1\": 0.95,\n",
        "    \"ranger_beta2\": 0.999,\n",
        "    \"ranger_eps\": 1e-5,\n",
        "    \"ranger_use_gc\": True,\n",
        "    \"ranger_gc_conv_only\": False,\n",
        "\n",
        "    # 스케줄러 설정\n",
        "    \"scheduler\": \"CosineAnnealingWarmRestarts\",  # 'LambdaLR', 'MultiplicativeLR', 'StepLR', 'CosineAnnealingLR', \n",
        "                                        #'ReduceLROnPlateau', 'CyclicLR', 'OneCycleLR', 'CosineAnnealingWarmRestarts'\n",
        "    \"verbose\": False,\n",
        "\n",
        "    # LambdaLR 설정\n",
        "    \"lambda_factor\": 0.95,\n",
        "    \"last_epoch\": -1,\n",
        "\n",
        "    # StepLR 설정\n",
        "    \"step_size\": 10,\n",
        "    \"gamma\": 0.5,\n",
        "\n",
        "    # ReduceLROnPlateau 설정\n",
        "    \"mode\": \"min\",\n",
        "\n",
        "    # CyclicLR 설정\n",
        "    \"base_lr\": 0.00005,\n",
        "    \"max_lr\": 0.0001,\n",
        "    \"step_size_up\": 5,\n",
        "\n",
        "    # OneCycleLR 설정\n",
        "    \"steps_per_epoch\": 10,\n",
        "    \"anneal_strategy\": \"linear\",\n",
        "\n",
        "    # CosineAnnealingLR 설정\n",
        "    \"T_max\": 50,\n",
        "    \"eta_min\": 0,\n",
        "\n",
        "    # CosineAnnealingWarmRestarts 설정\n",
        "    \"T_0\": 10,\n",
        "    \"T_mult\": 1,\n",
        "\n",
        "    # 추가 훈련 기법 설정\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"early_stopping_delta\": 0.001,\n",
        "    \"use_mixed_precision\": use_mixed_precision,\n",
        "    \"gradient_clip_val\": gradient_clip_val, #0이면 안함\n",
        "}\n",
        "config = Config(**config_manual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. wandb 로긴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use wandb-core, temporary for wandb's new backend  \n",
        "wandb.require(\"core\")\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 데이터 셋 준비 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 전체 데이터 훈련 및 외부 데이터 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 100480\n",
            "Test dataset size: 3140\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 준비\n",
        "train_ds, test_dataset = create_data_set(base_path, config.img_size)\n",
        "\n",
        "# DataLoader 정의\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# 데이터 로더를 GPU로 이동\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 트레이닝 데이터 일부로 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 100480\n",
            "Test dataset size: 3140\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 준비\n",
        "train_ds, test_dataset = create_data_set(base_path, config.img_size)\n",
        "train_size = int(0.9 * len(train_ds))\n",
        "val_size = len(train_ds) - train_size\n",
        "train_dataset, validation_dataset = random_split(train_ds, [train_size, val_size])\n",
        "# DataLoader 정의\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "validation_loader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# 데이터 로더를 GPU로 이동\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "validation_loader = DeviceDataLoader(validation_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 디버깅 용도로 미니셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1004\n",
            "Training dataset size: 100480\n",
            "Test dataset size: 3140\n"
          ]
        }
      ],
      "source": [
        "print(int(0.01 * len(train_ds)))\n",
        "train_ds, test_dataset = create_data_set(base_path, config.img_size)\n",
        "train_size = int(0.01 * len(train_ds))\n",
        "val_size = int(0.01 * len(train_ds))\n",
        "rem_size = len(train_ds) - train_size - val_size\n",
        "train_dataset, validation_dataset, rem_dataset = random_split(train_ds, [train_size, val_size, rem_size])\n",
        "# DataLoader 정의\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "# 데이터 로더를 GPU로 이동\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "validation_loader = DeviceDataLoader(validation_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 훈련 세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 손실함수, 옵티마이저, 스케줄러 확인 -> 실행 필요 없음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(SmoothFocalLoss(),\n",
              " Ranger (\n",
              " Parameter Group 0\n",
              "     N_sma_threshhold: 5\n",
              "     alpha: 0.5\n",
              "     betas: (0.95, 0.999)\n",
              "     eps: 1e-05\n",
              "     initial_lr: 0.001\n",
              "     k: 6\n",
              "     lr: 0.001\n",
              "     step_counter: 0\n",
              "     weight_decay: 1e-05\n",
              " ),\n",
              " <torch.optim.lr_scheduler.CosineAnnealingWarmRestarts at 0x1a55683a470>)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_func = get_loss_function(config)\n",
        "optimizer_choice = get_optimizer(model.parameters(), config)\n",
        "scheduler = get_scheduler(optimizer_choice, config)\n",
        "loss_func, optimizer_choice, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. 모델 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tiny_vit_5m_224.dist_in22k_ft_in1k)\n",
            "INFO:timm.models._hub:[timm/tiny_vit_5m_224.dist_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiny_vit_5m_224.dist_in22k_ft_in1k\n"
          ]
        }
      ],
      "source": [
        "# 모델 준비 -> 필수!!!!\n",
        "model = CustomModel(config).to(config.device)\n",
        "# 백본 확인\n",
        "print(config.model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. 훈련 준비 - 메모리 비워내기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocated: 6.21 GB\n",
            "Cached: 6.45 GB\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <훈련>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:8xcjhaq2) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8e1c7859d21497a8b291536749e8149",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>learning_rate</td><td>█▁</td></tr><tr><td>train_class_0_accuracy</td><td>▁█</td></tr><tr><td>train_class_10_accuracy</td><td>▁█</td></tr><tr><td>train_class_11_accuracy</td><td>█▁</td></tr><tr><td>train_class_12_accuracy</td><td>▁█</td></tr><tr><td>train_class_13_accuracy</td><td>▁▁</td></tr><tr><td>train_class_14_accuracy</td><td>▁▁</td></tr><tr><td>train_class_15_accuracy</td><td>▁█</td></tr><tr><td>train_class_16_accuracy</td><td>█▁</td></tr><tr><td>train_class_1_accuracy</td><td>▁▁</td></tr><tr><td>train_class_2_accuracy</td><td>█▁</td></tr><tr><td>train_class_3_accuracy</td><td>▁█</td></tr><tr><td>train_class_4_accuracy</td><td>█▁</td></tr><tr><td>train_class_5_accuracy</td><td>▁█</td></tr><tr><td>train_class_6_accuracy</td><td>▁█</td></tr><tr><td>train_class_7_accuracy</td><td>█▁</td></tr><tr><td>train_class_8_accuracy</td><td>▁█</td></tr><tr><td>train_class_9_accuracy</td><td>▁▁</td></tr><tr><td>train_f1_macro</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>train_lowest_class_accuracy</td><td>▁▁</td></tr><tr><td>valid_class_0_accuracy</td><td>▁█</td></tr><tr><td>valid_class_10_accuracy</td><td>▁█</td></tr><tr><td>valid_class_11_accuracy</td><td>▁█</td></tr><tr><td>valid_class_12_accuracy</td><td>▁▁</td></tr><tr><td>valid_class_13_accuracy</td><td>▁▁</td></tr><tr><td>valid_class_14_accuracy</td><td>▁▁</td></tr><tr><td>valid_class_15_accuracy</td><td>▁█</td></tr><tr><td>valid_class_16_accuracy</td><td>█▁</td></tr><tr><td>valid_class_1_accuracy</td><td>▁▁</td></tr><tr><td>valid_class_2_accuracy</td><td>▁█</td></tr><tr><td>valid_class_3_accuracy</td><td>▁█</td></tr><tr><td>valid_class_4_accuracy</td><td>█▁</td></tr><tr><td>valid_class_5_accuracy</td><td>▁█</td></tr><tr><td>valid_class_6_accuracy</td><td>▁█</td></tr><tr><td>valid_class_7_accuracy</td><td>▁█</td></tr><tr><td>valid_class_8_accuracy</td><td>█▁</td></tr><tr><td>valid_class_9_accuracy</td><td>▁▁</td></tr><tr><td>valid_f1_macro</td><td>▁█</td></tr><tr><td>valid_loss</td><td>█▁</td></tr><tr><td>valid_lowest_class_accuracy</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.00098</td></tr><tr><td>train_class_0_accuracy</td><td>0.43939</td></tr><tr><td>train_class_10_accuracy</td><td>0.06849</td></tr><tr><td>train_class_11_accuracy</td><td>0</td></tr><tr><td>train_class_12_accuracy</td><td>0.01562</td></tr><tr><td>train_class_13_accuracy</td><td>0</td></tr><tr><td>train_class_14_accuracy</td><td>0</td></tr><tr><td>train_class_15_accuracy</td><td>0.09722</td></tr><tr><td>train_class_16_accuracy</td><td>0.65574</td></tr><tr><td>train_class_1_accuracy</td><td>0</td></tr><tr><td>train_class_2_accuracy</td><td>0.5614</td></tr><tr><td>train_class_3_accuracy</td><td>0.26984</td></tr><tr><td>train_class_4_accuracy</td><td>0.58667</td></tr><tr><td>train_class_5_accuracy</td><td>0.63889</td></tr><tr><td>train_class_6_accuracy</td><td>0.65455</td></tr><tr><td>train_class_7_accuracy</td><td>0.20755</td></tr><tr><td>train_class_8_accuracy</td><td>0.14706</td></tr><tr><td>train_class_9_accuracy</td><td>0</td></tr><tr><td>train_f1_macro</td><td>0.20787</td></tr><tr><td>train_loss</td><td>0.07912</td></tr><tr><td>train_lowest_class_accuracy</td><td>0</td></tr><tr><td>valid_class_0_accuracy</td><td>0.755</td></tr><tr><td>valid_class_10_accuracy</td><td>0.07071</td></tr><tr><td>valid_class_11_accuracy</td><td>0.08</td></tr><tr><td>valid_class_12_accuracy</td><td>0</td></tr><tr><td>valid_class_13_accuracy</td><td>0</td></tr><tr><td>valid_class_14_accuracy</td><td>0</td></tr><tr><td>valid_class_15_accuracy</td><td>0.26904</td></tr><tr><td>valid_class_16_accuracy</td><td>0.94472</td></tr><tr><td>valid_class_1_accuracy</td><td>0</td></tr><tr><td>valid_class_2_accuracy</td><td>0.80597</td></tr><tr><td>valid_class_3_accuracy</td><td>0.23196</td></tr><tr><td>valid_class_4_accuracy</td><td>0.82938</td></tr><tr><td>valid_class_5_accuracy</td><td>1</td></tr><tr><td>valid_class_6_accuracy</td><td>0.7734</td></tr><tr><td>valid_class_7_accuracy</td><td>0.18274</td></tr><tr><td>valid_class_8_accuracy</td><td>0.05854</td></tr><tr><td>valid_class_9_accuracy</td><td>0</td></tr><tr><td>valid_f1_macro</td><td>0.29119</td></tr><tr><td>valid_loss</td><td>0.06518</td></tr><tr><td>valid_lowest_class_accuracy</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">splendid-serenity-270</strong> at: <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/8xcjhaq2' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/8xcjhaq2</a><br/> View project at: <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240807_033730-8xcjhaq2\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:8xcjhaq2). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\hhh\\Desktop\\AI_LAB\\스터디그룹_doc_clafssification\\wandb\\run-20240807_034317-96va6ib1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/96va6ib1' target=\"_blank\">pretty-monkey-271</a></strong> to <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/96va6ib1' target=\"_blank\">https://wandb.ai/upstage6_doc_classification/k-fold-cross-validation-pytorch/runs/96va6ib1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06b4b815bb564509ae8d30ccdada55db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd75746d9dbc45869a089e97f3b52d21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28abdb55a98d484884a3fccfcab8aafe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/393 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 - Loss: 0.0577, F1 Macro: 0.3706, Lowest_class_accuracy: 0.0000, Val Loss: 0.0451, Val F1 Macro: 0.4652, Val Lowest_class_accuracy: 0.0000, Time: 145.79s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03fe698e65ab47e282f1061a79492f12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de3a76a460ba4f7e9173a3230d5d863f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/393 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/300 - Loss: 0.0508, F1 Macro: 0.4324, Lowest_class_accuracy: 0.0000, Val Loss: 0.0448, Val F1 Macro: 0.5269, Val Lowest_class_accuracy: 0.0000, Time: 153.79s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a703a0ac0c4e4993b3d338cde2a45bcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e97a679df6114165839f7683b70e96cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/393 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/300 - Loss: 0.0461, F1 Macro: 0.4451, Lowest_class_accuracy: 0.0000, Val Loss: 0.0435, Val F1 Macro: 0.5308, Val Lowest_class_accuracy: 0.0000, Time: 143.28s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29c7c951d7424ee286f630b2451e5ccc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4014f90df2f84477882d5e85eb1c6a29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/393 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/300 - Loss: 0.0386, F1 Macro: 0.5384, Lowest_class_accuracy: 0.0000, Val Loss: 0.0372, Val F1 Macro: 0.5843, Val Lowest_class_accuracy: 0.0000, Time: 155.84s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c97741011c804a8baab8c75ce3f2f2b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1850b1c59ac74b3c864d9775a16aefcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/393 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/300 - Loss: 0.0343, F1 Macro: 0.5867, Lowest_class_accuracy: 0.0000, Val Loss: 0.0330, Val F1 Macro: 0.6242, Val Lowest_class_accuracy: 0.0000, Time: 152.16s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "606789140f7040a7a42e183f61083e4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d06d49a64235466c9928fdaffaa5aa5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/393 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/300 - Loss: 0.0326, F1 Macro: 0.6261, Lowest_class_accuracy: 0.0000, Val Loss: 0.0301, Val F1 Macro: 0.6579, Val Lowest_class_accuracy: 0.0000, Time: 132.10s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c18bb65b9c3e4f62b7c68dc422d0bf56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 7/300:   0%|          | 0/126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[152], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 일반 훈련\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, config)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_validate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#validate 안하려면 False로\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[151], line 186\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_loader, validation_loader, is_validate)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_mixed_precision:\n\u001b[1;32m--> 186\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch_mixed_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch_full_precision(train_loader, epoch)\n",
            "Cell \u001b[1;32mIn[151], line 128\u001b[0m, in \u001b[0;36mTrainer.train_epoch_mixed_precision\u001b[1;34m(self, train_loader, epoch)\u001b[0m\n\u001b[0;32m    126\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m--> 128\u001b[0m     loss, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgradient_clip_val \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[1;32mIn[115], line 49\u001b[0m, in \u001b[0;36mCustomModel.training_step\u001b[1;34m(self, batch, loss_func)\u001b[0m\n\u001b[0;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m cutmix_criterion(loss_func, logits, y_a, y_b, lam)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(logits, y)\n\u001b[0;32m     51\u001b[0m preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[115], line 33\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(features)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\tiny_vit.py:552\u001b[0m, in \u001b[0;36mTinyVit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 552\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\tiny_vit.py:544\u001b[0m, in \u001b[0;36mTinyVit.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    542\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages, x)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 544\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\tiny_vit.py:410\u001b[0m, in \u001b[0;36mTinyVitStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    408\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[0;32m    409\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# BCHW -> BHWC\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# BHWC -> BCHW\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\tiny_vit.py:332\u001b[0m, in \u001b[0;36mTinyVitBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    329\u001b[0m x \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(x)\n\u001b[0;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 332\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, C, L)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    335\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x))\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hhh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 일반 훈련\n",
        "trainer = Trainer(model, config)\n",
        "trainer.train(train_loader, test_loader, is_validate = True) #validate 안하려면 False로"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 체크포인트, 연속 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "체크 포인트 경로 (상대경로)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_path = 'models/tiny_vit_11m_224_Ranger_hzeqx347/checkpoint_epoch_36.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "처음부터 트레이닝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(model, config) #아무거나 불러와도 됨\n",
        "trainer.setup_training(checkpoint_path= checkpoint_path, continue_training=False)\n",
        "trainer.train(train_loader, validation_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이어서 트레이닝 (옵티마이저, 스케줄러 불러옴)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 체크포인트에서 훈련 계속 (10 에폭 추가)\n",
        "trainer = Trainer(model, config)\n",
        "trainer.setup_training(checkpoint_path= checkpoint_path, continue_training=True, additional_epochs=10)\n",
        "trainer.train(train_loader, validation_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 모델 분석 및 결과제출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 분석 및 결과제출 - 특정 에포크 모델 불러오기\n",
        "- 저장 위치로 불러온다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 저장 위치\n",
        "model_path = 'models/caformer_m36_Ranger_1zo3gt64/checkpoint_epoch_10.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unexpected keys (head.fc.fc1.bias, head.fc.fc1.weight, head.fc.norm.bias, head.fc.norm.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set state called\n",
            "Model loaded from models/caformer_m36_Ranger_1zo3gt64/checkpoint_epoch_10.pt\n",
            "Continuing training from epoch 10\n",
            "Best validation F1 score: 0.9992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 785/785 [05:36<00:00,  2.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions for method: no_tta\n",
            "Saved label probabilities for method: no_tta\n",
            "Saved predictions for method: mean\n",
            "Saved label probabilities for method: mean\n",
            "Saved predictions for method: max\n",
            "Saved label probabilities for method: max\n",
            "Saved predictions for method: temp_sharpen\n",
            "Saved label probabilities for method: temp_sharpen\n",
            "Saved predictions for method: mode\n",
            "Saved label probabilities for method: mode\n",
            "Saved predictions for method: modethreshold\n",
            "Saved label probabilities for method: modethreshold\n",
            "Saved predictions for method: ensemble\n"
          ]
        }
      ],
      "source": [
        "# 분석 예시\n",
        "trainer = Trainer(model, config) #아무거나 사용해도 됨 , 사용한 모델과 컨피그는 아무거나\n",
        "trainer.load_model(model_path)\n",
        "tta = TTA(trainer.model, trainer.config)\n",
        "\n",
        "#test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 테스트 데이터에 대한 예측 및 제출 파일 생성\n",
        "tta.automatic_tta_submission(test_loader, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 분석 및 결과제출 - 베스트 모델 불러오기\n",
        "- 그룹 이름으로 불러온다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 이름 (폴더명임)\n",
        "model_name_anal = 'tiny_vit_11m_224_Ranger_4j1cx18a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(model, config) #아무거나 사용해도 됨 , 사용한 모델과 컨피그는 아무거나\n",
        "trainer.load_best_model(group_name=model_name_anal)\n",
        "tta = TTA(trainer.model, trainer.config)\n",
        "\n",
        "#test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 테스트 데이터에 대한 예측 및 제출 파일 생성\n",
        "tta.automatic_tta_submission(test_loader, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions:  16%|█▋        | 4093/25120 [29:23<2:27:46,  2.37it/s]"
          ]
        }
      ],
      "source": [
        "# 검증 데이터에 대한 TTA 분석\n",
        "disagreement_dataset = tta.analyze_tta(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Focal Alpha 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Focal Weight 업데이트\n",
        "error_rates = [0.115, 0.115, 0.106, 0.021, 0.018, 0.014, 0.011, 0.011, 0.004, 0.004, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "max_error = max(error_rates)\n",
        "alphas = [min(1.0, (rate / max_error) * 2) for rate in error_rates]\n",
        "alphas = [max(0.1, alpha) for alpha in alphas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 추가 구축 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3,4,7,14\n",
        "오분류한 데이터들 모으기 -> 증강 -> 학습\n",
        "(기존모델 fine tune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SIMCLR -> 자주 틀리는 라벨에 한해서 구현하면 될듯 하다. 그리고 그 라벨로 예측한 것에 2차분류기를 simclr로 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Knowledge Distillation 크고 좋은 모델을 날잡고 훈련해서 knowledge distillation을 시도해봐도 좋을듯 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2차분류기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Progressive Resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adversarial Training (성능 약화 예상)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convolutional Block Attention Module (CBAM) -> 해볼 가치 충분. CNN 기반에 잘 돌아감"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Squeeze-and-Excitation (SE) 블록"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
